<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Topics in Classification</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2022-01-02" />
    <script src="u3_d02-classtopics_files/header-attrs-2.11/header-attrs.js"></script>
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Topics in Classification

### Applications of Data Science - Class 13

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2022-01-02

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2022.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# Topics in Classification

---

# Life isn't perfect

Let's tackle just a few issues:

- Not enough labelled data and data labeling is expensive
- Imbalanced Classes

---

class: section-slide

# Active Learning

---

### Got Data?




```r
n &lt;- 20
x1 &lt;- rnorm(n, 0, 1); x2 &lt;- rnorm(n, 0, 1)
t &lt;- 2 - 4 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))
glm_mod &lt;- glm(y ~ x1 + x2, family = "binomial")
```

&lt;img src="images/AL-LR-Example-1.png" width="50%" /&gt;

---

### Want more?

&gt; The key idea behind *active learning* is that a machine learning algorithm can
achieve greater accuracy with fewer training labels if it is allowed to choose the
data from which it learns. An active learner may pose *queries*, usually in the form
of unlabeled data instances to be labeled by an *oracle* (e.g., a human annotator).
Active learning is well-motivated in many modern machine learning problems,
where unlabeled data may be abundant or easily obtained, but labels are difficult,
time-consuming, or expensive to obtain.

([Settles, 2010](http://burrsettles.com/pub/settles.activelearning.pdf))

&gt; You want data? Well data costs!

(No one, ever)

---

### Where this is going

&lt;img src="images/active_learning_plan.png" style="width: 90%" /&gt;

---

### Active Learning Scenarios

1. **Membership Query Synthesis**: You get to choose which (maybe theoretical) points you'd want `\(y\)` labelled for.
2. **Stream-Based Selective Sampling**: You get 1 point at a time and decide which ones you'd like to query and which to discard.
3. **Pool-Based Sampling**: You have a large collecetion of unlabelled points at your disposal, you need to send the "best ones" for labelling

&lt;img src="images/active_learning_scenarios.png" style="width: 70%" /&gt;

---

### Uncertainty Sampling

.insight[
üí° For a 2-class dataset, the observations your model is most uncertain of are...
]

&lt;img src="images/AL-LR-Example2-1.png" width="50%" /&gt;

---

### Uncertainty Sampling Measures

Let `\(\hat{y}_i\)` be the predicted classes with `\(i\)`th highest score (probability), for observations `\(x\)` under some model `\(\theta\)`.

So `\(\hat{y}_1 = \arg\max{P_{\theta}(y|x)}\)` are the actual predicted classes, `\(\hat{y}_2\)` are the second choices, etc.

* Least Confidence: Choose those observations for which `\(P_{\theta}(\hat{y}_1|x)\)` is smallest:

`\(x^*_{LC} = \arg\min{P_{\theta}(\hat{y}_1|x)}\)`

.insight[
üí° For a 2-class balanced dataset, this means...
]

---

* Margin Sampling: Choose those observations for which the margin between the two highest scores is smallest:

`\(x^*_M = \arg\min{P_{\theta}(\hat{y}_1|x) - P_{\theta}(\hat{y}_2|x)}\)`

.insight[
üí° For a 2-class balanced dataset, this means...
]

* Entropy: Choose the observations for which entropy is highest:

`\(x^*_H = \arg\max-{\sum_i P_{\theta}(\hat{y}_i|x) \log[P_{\theta}(\hat{y}_i|x)]}\)`

We will talk more about entropy in Neural Networks, let's minimize negative entropy.

.insight[
üí° For a 2-class balanced dataset, this means...
]

---

### Example: The `spotify_songs` data from HW3


```r
spotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')

spotify_songs %&gt;% count(playlist_genre)
```

```
## # A tibble: 6 x 2
##   playlist_genre     n
##   &lt;chr&gt;          &lt;int&gt;
## 1 edm             6043
## 2 latin           5155
## 3 pop             5507
## 4 r&amp;b             5431
## 5 rap             5746
## 6 rock            4951
```

Let's try to classify the genre of a song!

---

We'll take only the 12 audio features as predictors, and choose each `track_id` once (remember each song appears a few times?):


```r
library(tidymodels)

predictors &lt;- 12:23

spotify_songs &lt;- spotify_songs %&gt;%
  group_by(track_id) %&gt;%
  slice_sample(n = 1) %&gt;%
  ungroup() %&gt;%
  distinct(track_name, .keep_all = TRUE) %&gt;%
  select(track_id, track_name, track_artist, playlist_genre, predictors) %&gt;%
  mutate(playlist_genre = recode(playlist_genre, "r&amp;b" = "rnb"))

set.seed(76)
sptfy_split_obj &lt;- spotify_songs %&gt;%
  initial_split(prop = 0.8)
sptfy_tr &lt;- training(sptfy_split_obj)
sptfy_te &lt;- testing(sptfy_split_obj)
```

---

Plot twist! We only have 20 songs from each genre!


```r
set.seed(1)
sptfy_tr_small &lt;- sptfy_tr %&gt;%
  group_by(playlist_genre) %&gt;%
  slice_sample(n = 20) %&gt;%
  ungroup()

sptfy_tr_small %&gt;% count(playlist_genre)
```

```
## # A tibble: 6 x 2
##   playlist_genre     n
##   &lt;chr&gt;          &lt;int&gt;
## 1 edm               20
## 2 latin             20
## 3 pop               20
## 4 rap               20
## 5 rnb               20
## 6 rock              20
```

Muhaha!

---

We'll also have a pool of songs to query, `sptfy_tr_large`:


```r
sptfy_tr_large &lt;- sptfy_tr %&gt;%
  anti_join(sptfy_tr_small, by = "track_id")
```

We `bake()` the 3 datasets with the small sample params recipe:


```r
sptfy_rec &lt;- recipe(playlist_genre ~ ., data = sptfy_tr_small) %&gt;%
  update_role(track_id, track_name, track_artist,
              new_role = "id") %&gt;%
  step_normalize(all_numeric(), -has_role("id")) %&gt;%
  step_string2factor(playlist_genre) %&gt;%
  prep(sptfy_tr_small, strings_as_factors = FALSE)

sptfy_tr_small &lt;- juice(sptfy_rec)
sptfy_tr_large &lt;- bake(sptfy_rec, new_data = sptfy_tr_large)
sptfy_te &lt;- bake(sptfy_rec, new_data = sptfy_te)
```

---

Let's build a simple GBT model:


```r
mod_spec &lt;- boost_tree(mode = "classification", trees = 100) %&gt;%
  set_engine("xgboost", eval_metric = "mlogloss")

mod_fit &lt;- mod_spec %&gt;%
  fit(playlist_genre ~ ., data = sptfy_tr_small %&gt;%
        select(-track_id, -track_name, -track_artist))

mod_pred &lt;- mod_fit %&gt;%
  predict(new_data = sptfy_tr_large, type = "prob")

mod_pred
```

```
## # A tibble: 18,640 x 6
##    .pred_edm .pred_latin .pred_pop .pred_rap .pred_rnb .pred_rock
##        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
##  1   0.851      0.0785     0.0531    0.0105   0.00253     0.00413
##  2   0.0673     0.0383     0.123     0.00895  0.0165      0.746  
##  3   0.00363    0.0233     0.00457   0.962    0.00426     0.00192
##  4   0.00710    0.612      0.321     0.0322   0.0157      0.0118 
##  5   0.00596    0.000497   0.0213    0.00191  0.000541    0.970  
##  6   0.456      0.0792     0.179     0.261    0.00740     0.0175 
##  7   0.933      0.0141     0.0363    0.00172  0.00598     0.00926
##  8   0.0807     0.464      0.246     0.0934   0.106       0.00940
##  9   0.0225     0.00112    0.933     0.00173  0.0301      0.0117 
## 10   0.129      0.367      0.379     0.0908   0.00663     0.0277 
## # ... with 18,630 more rows
```

---

Test accuracy?


```r
mod_te_pred_class &lt;- mod_fit %&gt;%
      predict(new_data = sptfy_te) %&gt;%
      bind_cols(sptfy_te)

mod_te_pred_class %&gt;%
  accuracy(truth = playlist_genre, estimate = .pred_class)
```

```
## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.380
```




Remember this model was built on 120 of almost 19K available unique songs!

---

Test Recall and Precision:


```r
mod_te_pred_class %&gt;%
  group_by(playlist_genre) %&gt;%
  accuracy(truth = playlist_genre, estimate = .pred_class) %&gt;%
  select(playlist_genre, recall = .estimate) %&gt;%
  bind_cols(
    mod_te_pred_class %&gt;%
      group_by(.pred_class) %&gt;%
      accuracy(truth = playlist_genre, estimate = .pred_class) %&gt;%
      select(precision = .estimate)
  )
```

```
## # A tibble: 6 x 3
##   playlist_genre recall precision
##   &lt;fct&gt;           &lt;dbl&gt;     &lt;dbl&gt;
## 1 edm             0.556     0.550
## 2 latin           0.262     0.269
## 3 pop             0.236     0.231
## 4 rap             0.438     0.453
## 5 rnb             0.278     0.281
## 6 rock            0.459     0.438
```

---

Build a function which will take each row of predicted probs and return a list of 3 uncertainty metrics:


```r
uncertainty_lc &lt;- function(probs) {
  max(probs)
}

uncertainty_m &lt;- function(probs) {
  o &lt;- order(probs, decreasing = TRUE)
  probs[o[1]] - probs[o[2]]
}

uncertainty_h &lt;- function(probs) {
  sum(probs * log(probs + 0.000001))
}

uncertainty &lt;- function(...) {
  probs &lt;- c(...)
  list(
    lc = uncertainty_lc(probs),
    margin = uncertainty_m(probs),
    entropy = uncertainty_h(probs)
  )
}
```

---


```r
mod_unc &lt;- mod_pred %&gt;% pmap_dfr(uncertainty)

mod_unc
```

```
## # A tibble: 18,640 x 3
##       lc margin entropy
##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1 0.851 0.773   -0.578
##  2 0.746 0.623   -0.893
##  3 0.962 0.939   -0.205
##  4 0.612 0.291   -0.929
##  5 0.970 0.948   -0.162
##  6 0.456 0.195   -1.32 
##  7 0.933 0.896   -0.330
##  8 0.464 0.218   -1.41 
##  9 0.933 0.903   -0.326
## 10 0.379 0.0123  -1.35 
## # ... with 18,630 more rows
```

Obviously these are correlated:

---

.pull-left[


```r
mod_unc %&gt;% slice_sample(n = 1000) %&gt;%
  ggplot(aes(lc, margin)) +
  geom_point() +
  theme_light() +
  theme(text =
          element_text(size = 14))
```

&lt;img src="images/AL-MR-Lc-Margin-1.png" width="100%" /&gt;

]

.pull-right[


```r
mod_unc %&gt;% slice_sample(n = 1000) %&gt;%
  ggplot(aes(lc, entropy)) +
  geom_point() +
  theme_light() +
  theme(text =
          element_text(size = 14))
```

&lt;img src="images/AL-MR-Lc-Entropy-1.png" width="100%" /&gt;

]

---

Which are the top 10 songs in terms of each metric the model is most curious about?


```r
sptfy_tr_large_with_unc &lt;- sptfy_tr_large %&gt;%
  bind_cols(mod_unc) %&gt;%
  select(track_name, track_artist, playlist_genre, lc, margin, entropy)

sptfy_tr_large_with_unc %&gt;%
  slice_min(lc, n = 10) %&gt;%
  arrange(lc, track_name)
```

```
## # A tibble: 10 x 6
##    track_name                track_artist   playlist_genre    lc  margin entropy
##    &lt;chr&gt;                     &lt;chr&gt;          &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 Good Thing                Tritonal       edm            0.208 5.76e-3   -1.76
##  2 &lt;U+042F&gt; &lt;U+0441&gt;&lt;U+043C&gt;&lt;U+043E&gt;&lt;U+0442&gt;&lt;U+0440&gt;&lt;U+044E&gt; &lt;U+043D&gt;&lt;U+0430&gt; &lt;U+043D&gt;&lt;U+0435&gt;&lt;U+0431&gt;&lt;U+043E&gt;          Basta          rap            0.211 1.55e-2   -1.78
##  3 Eenie Meenie              Justin Bieber  latin          0.215 1.40e-2   -1.77
##  4 Can't Stop Me (feat. She~ Afrojack       edm            0.216 2.10e-2   -1.73
##  5 Revival                   Gregory Porter pop            0.222 1.04e-2   -1.72
##  6 Take It To Reality        Alison Wonder~ pop            0.223 2.58e-2   -1.77
##  7 Drops                     Solarrio       rnb            0.224 1.52e-2   -1.72
##  8 Fr√≠a Como El Viento       Luis Miguel    latin          0.224 7.62e-5   -1.72
##  9 Tu No Eres Para Mi        Fanny Lu       latin          0.224 2.27e-2   -1.75
## 10 Symphony (feat. Zara Lar~ Clean Bandit   edm            0.224 1.56e-4   -1.72
```

---


```r
sptfy_tr_large_with_unc %&gt;%
  slice_min(margin, n = 10) %&gt;%
  arrange(margin, track_name)
```

```
## # A tibble: 10 x 6
##    track_name               track_artist    playlist_genre    lc  margin entropy
##    &lt;chr&gt;                    &lt;chr&gt;           &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 Back in My Life          Alice DJ        pop            0.438 1.99e-5   -1.05
##  2 Fr√≠a Como El Viento      Luis Miguel     latin          0.224 7.62e-5   -1.72
##  3 Into The Night           Benny Mardones  rock           0.226 1.01e-4   -1.75
##  4 El Taxi                  Pitbull         latin          0.332 1.01e-4   -1.23
##  5 Symphony (feat. Zara La~ Clean Bandit    edm            0.224 1.56e-4   -1.72
##  6 I Can't Get No           JETFIRE         edm            0.407 1.63e-4   -1.28
##  7 Two Fine People          Yusuf / Cat St~ rock           0.228 1.95e-4   -1.68
##  8 Y Si Te Digo - Version ~ Fanny Lu        latin          0.306 2.19e-4   -1.40
##  9 Under My Thumb           The Rolling St~ rock           0.356 2.40e-4   -1.29
## 10 Vente Pa' Ca             Ricky Martin    latin          0.322 2.49e-4   -1.54
```

---


```r
sptfy_tr_large_with_unc %&gt;%
  slice_min(entropy, n = 10) %&gt;%
  arrange(entropy, track_name)
```

```
## # A tibble: 10 x 6
##    track_name             track_artist      playlist_genre    lc  margin entropy
##    &lt;chr&gt;                  &lt;chr&gt;             &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 &lt;U+042F&gt; &lt;U+0441&gt;&lt;U+043C&gt;&lt;U+043E&gt;&lt;U+0442&gt;&lt;U+0440&gt;&lt;U+044E&gt; &lt;U+043D&gt;&lt;U+0430&gt; &lt;U+043D&gt;&lt;U+0435&gt;&lt;U+0431&gt;&lt;U+043E&gt;       Basta             rap            0.211 1.55e-2   -1.78
##  2 Eenie Meenie           Justin Bieber     latin          0.215 1.40e-2   -1.77
##  3 Take It To Reality     Alison Wonderland pop            0.223 2.58e-2   -1.77
##  4 Good Thing             Tritonal          edm            0.208 5.76e-3   -1.76
##  5 Tu No Eres Para Mi     Fanny Lu          latin          0.224 2.27e-2   -1.75
##  6 Into The Night         Benny Mardones    rock           0.226 1.01e-4   -1.75
##  7 Cold Ain't For Me      Oceans            pop            0.250 4.65e-2   -1.75
##  8 Come Back              Green Tea         rnb            0.260 6.29e-2   -1.74
##  9 Fortunate Son          Creedence Clearw~ rock           0.235 3.61e-2   -1.74
## 10 Never Getting Over You Lost Stars        pop            0.257 5.94e-2   -1.74
```

---

So far it's only interesting. Will sending the observations our model is most curious about to the "oracle" prove to increase test accuracy better than random observations? See full code in slides Rmd files.





&lt;img src="images/Simul-Unc-1.png" width="100%" /&gt;

---

### Query by Commity (QBC)

Similar to ensemble models, we have a committee of models:

`\(C = \{\theta_1, ..., \theta_C\}\)`

Which observations the commitee is most uncertain of? E.g.

`\(x^*_{VE} = \arg\max-{\sum_i \frac{V(\hat{y}_i|x)}{|C|}\log{\frac{V(\hat{y}_i|x)}{|C|}}}\)`

Where `\(V(\hat{y}_i|x)\)` is the number of votes for `\(\hat{y}_i\)`.

How do you get a committee?
- Different models
- Bagging
- Same model, different subsets of features
- Same model, different params

---

Let's do 6 GBT models, each receiving 2 different consecutive features:


```r
fit_sub_model &lt;- function(i, tr, te) {
  mod_fit &lt;- mod_spec %&gt;%
    fit(playlist_genre ~ ., data = tr %&gt;%
          select(playlist_genre, (2 + i * 2):(3 + i * 2)))

  mod_fit %&gt;%
    predict(new_data = te)
}
mod_pred &lt;- map_dfc(1:6, fit_sub_model,
                    tr = sptfy_tr_small, te = sptfy_tr_large)
mod_pred
```

```
## # A tibble: 18,640 x 6
##    .pred_class...1 .pred_class...2 .pred_class...3 .pred_class...4
##    &lt;fct&gt;           &lt;fct&gt;           &lt;fct&gt;           &lt;fct&gt;          
##  1 rap             pop             rnb             pop            
##  2 rnb             edm             edm             pop            
##  3 rock            rap             edm             rap            
##  4 rock            edm             rnb             rnb            
##  5 rock            latin           pop             rnb            
##  6 edm             rnb             rap             pop            
##  7 pop             rnb             pop             edm            
##  8 latin           rnb             edm             rap            
##  9 edm             rap             edm             pop            
## 10 latin           edm             pop             pop            
## # ... with 18,630 more rows, and 2 more variables: .pred_class...5 &lt;fct&gt;,
## #   .pred_class...6 &lt;fct&gt;
```

---


```r
mod_qbc &lt;- mod_pred %&gt;%
  mutate(probs = pmap(
    select(., starts_with(".pred")),
    function(...) table(c(...)) / 6),
    vote_entropy = map_dbl(probs, uncertainty_h),
    vote_margin = map_dbl(probs, uncertainty_m))

sptfy_tr_large_with_qbc &lt;- sptfy_tr_large %&gt;%
  bind_cols(mod_qbc) %&gt;%
  select(track_name, track_artist, playlist_genre,
         starts_with(".pred"), vote_entropy)

sptfy_tr_large_with_qbc %&gt;%
  slice_min(vote_entropy, n = 10) %&gt;%
  arrange(vote_entropy) %&gt;%
  select(starts_with(".pred"))
```

```
## # A tibble: 278 x 6
##    .pred_class...17 .pred_class...18 .pred_class...19 .pred_class...20
##    &lt;fct&gt;            &lt;fct&gt;            &lt;fct&gt;            &lt;fct&gt;           
##  1 edm              pop              rap              rnb             
##  2 rock             rap              edm              rnb             
##  3 rock             pop              rap              edm             
##  4 latin            rock             pop              edm             
##  5 edm              pop              rap              latin           
##  6 edm              pop              latin            rnb             
##  7 edm              latin            pop              rap             
##  8 rock             rnb              edm              latin           
##  9 rnb              rock             rap              latin           
## 10 rock             rap              latin            pop             
## # ... with 268 more rows, and 2 more variables: .pred_class...21 &lt;fct&gt;,
## #   .pred_class...22 &lt;fct&gt;
```

---

Will sending the observations our committee is in most disagreement about to the "oracle" prove to increase test accuracy better than random observations? See full code in slides Rmd files.





&lt;img src="images/Simul-QBC-1.png" width="100%" /&gt;

---

### Other Active Learning Metrics

- Expected Model Change
- Expected Error Reduction
- Variance Reduction
- And more...

---

class: section-slide

# Imbalanced Classes

---

### Typical examples of Imbalanced Classes scenarios

- Rare diseases: [this](https://www.kaggle.com/c/hivprogression) dataset contains genetic data for 1,000 HIV patients, 206 out of 1,000 patients improved after 16 weeks of therapy
- Conversion/Sell/CTR rates: [this](https://www.kaggle.com/c/avazu-ctr-prediction) dataset contains 10 days of Click-Through-Rate data for Avazu mobile ads, ~6.8M clicked out of ~40.4M
- Fraud detection: [this](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset contains credit card transactions for a major European CC, 492 frauds out of 284,807 transactions

---

### What's so difficult about imbalanced classes?


```r
okcupid_pets &lt;- as_tibble(read_rds("../data/okcupid3_imp_mice.rds"))

idx &lt;- read_rds("../data/okcupid3_idx.rda")
train_idx &lt;- idx$train_idx
valid_idx &lt;- idx$valid_idx
test_idx &lt;- idx$test_idx

ok_train &lt;- okcupid_pets[train_idx, ]
ok_valid &lt;- okcupid_pets[valid_idx, ]

ok_train %&gt;%
  count(pets) %&gt;%
  mutate(pct = round(n / sum(n), 2))
```

```
## # A tibble: 2 x 3
##   pets      n   pct
##   &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;
## 1 cats   1624  0.16
## 2 dogs   8376  0.84
```

.insight[
üí° What's a sure way to get 84% accuracy?
]

---




```r
mod_glm &lt;- glm(pets ~ ., data = ok_train, family = "binomial")
pred_glm &lt;- 1 - predict(mod_glm, ok_valid, type = "response")

pred_glm_class &lt;- ifelse(pred_glm &gt; 0.5, "cats", "dogs")
true_class &lt;- ok_valid$pets

table(true_class, pred_glm_class)
```

```
##           pred_glm_class
## true_class cats dogs
##       cats   66  407
##       dogs   62 2194
```

```r
report_accuracy_and_auc(true_class, pred_glm)
```

```
## Setting direction: controls &lt; cases
```

```
## AUC: 0.736
## ACC: 0.828
## Cats: Recall: 0.14
##       Precision: 0.516
## Dogs: Recall: 0.973
##       Precision: 0.844
```

---

### Remedies for Imbalanced Classes

- Model level
  - Tuning parameters and Cutoff choice
  - Cost-aware training: Case weights and Prior probabilities
- Data level
  - Down sampling
  - Up sampling
  - Get more data and features from minority class, similar to Active Learning
- Change of Framework
  - Anomaly Detection
- One final word of wisdom

---

### Tuning parameters and cutoff choice

A general good approach would be:

1. Choose a  model to maximize AUC on one part of the training dataset (using resampling)
2. Choose a cutoff score on another part of the training dataset
3. Fitting the entire thing on all training set and checking on test set

But. You could incorporate your initial goal even into (1), making the cutoff another tuning parameter that would maximize:

- Recall(cats): If never missing a cat person (the minority class) is your job .font80percent[(while maintaining acceptable level of Precision(cats))]
- Precision(cats): If you don't have room for error when you say a person is a cat person .font80percent[(while maintaining acceptable level of Precision(cats))]
- Some other metric like F1-score

---

Let us choose a model by maximizing AUC then present our client with a few potential cutoffs.

We'll begin by splitting our training set into two:


```r
ok_split &lt;- initial_split(ok_train, prop = 0.7, strata = pets)

ok_train1 &lt;- training(ok_split)
ok_train2 &lt;- testing(ok_split)

dim(ok_train1)
```

```
## [1] 6999   38
```

```r
dim(ok_train2)
```

```
## [1] 3001   38
```

---

Use the first training set to choose a GBT model to maximize AUC with 5-fold CV:


```r
mod_gbt_spec &lt;- boost_tree(mode = "classification",
                           mtry = tune(),
                           min_n = tune(),
                           learn_rate = tune(),
                           trees = 1000) %&gt;%
  set_engine("xgboost", eval_metric = "logloss")

gbt_grid &lt;- grid_regular(mtry(range(10, 50)),
                         min_n(range(10, 100)),
                         learn_rate(range(-3, -1)),
                         levels = 3)
rec_gbt &lt;- recipe(pets ~ ., data = ok_train1) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
  prep(ok_train1)

cv_splits &lt;- vfold_cv(juice(rec_gbt), v = 5, strata = pets)
```

---


```r
tune_res &lt;- tune_grid(object = mod_gbt_spec,
                      preprocessor = recipe(pets~., data = juice(rec_gbt)),
                      resamples = cv_splits,
                      grid = gbt_grid,
                      control = control_grid(verbose = TRUE),
                      metrics = metric_set(roc_auc))
```

&lt;img src="images/Imbalanced-AUC-Res-1.png" width="100%" /&gt;



---

Fit the best model on all of `ok_train1` and get scores on `ok_train2`:


```r
mod_gbt_spec &lt;- mod_gbt_spec %&gt;%
  update(mtry = 30, trees = 1000,
         min_n = 10, learn_rate = 0.01)
mod_gbt &lt;- mod_gbt_spec %&gt;%
  fit(pets ~ ., data = juice(rec_gbt))

pred_gbt &lt;- mod_gbt %&gt;%
  predict(new_data = bake(rec_gbt, ok_train2), type = "prob") %&gt;%
  pull(.pred_cats)
```

---

You can use the ROC curve to understad the behavior of the cutoff:





&lt;img src="images/Imbalanced-AUC-ROC-1.png" width="60%" /&gt;

---

Maybe better, draw a histogram of `cats` score and mark the cutoffs there:

&lt;img src="images/Imbalanced-Score-Host-1.png" width="100%" /&gt;

---

Lastly, train on entire training set and evaluate on test set:


```r
mod_gbt &lt;- mod_gbt_spec %&gt;%
  fit(pets ~ ., data = bake(rec_gbt, ok_train))
pred_gbt &lt;- mod_gbt %&gt;%
  predict(new_data = bake(rec_gbt, ok_valid), type = "prob") %&gt;%
  pull(.pred_cats)
pred_gbt_class &lt;- ifelse(pred_gbt &gt; 0.17, "cats", "dogs")
true_class &lt;- ok_valid$pets
table(true_class, pred_gbt_class)
```

```
##           pred_gbt_class
## true_class cats dogs
##       cats  307  166
##       dogs  649 1607
```

```r
report_accuracy_and_auc(true_class, pred_gbt, cutoff = 0.17)
```

```
## AUC: 0.746
## ACC: 0.701
## Cats: Recall: 0.649
##       Precision: 0.321
## Dogs: Recall: 0.712
##       Precision: 0.906
```

---

### Cost aware training: Case weights

For example, in `glm()` you can simply specify a `weights` param:

&gt; when the elements of weights are positive integers *w_i*, each response *y_i* is the mean of *w_i* unit-weight observations


```r
pets_weights &lt;- rep(1, nrow(ok_train))
pets_weights[which(ok_train$pets == "cats")] &lt;- 5

mod_glm &lt;- glm(pets ~ ., data = ok_train,
*              family = "binomial", weights = pets_weights)

pred_glm &lt;- 1 - predict(mod_glm, ok_valid, type = "response")

pred_glm_class &lt;- ifelse(pred_glm &gt; 0.5, "cats", "dogs")
true_class &lt;- ok_valid$pets
```

---


```r
table(true_class, pred_glm_class)
```

```
##           pred_glm_class
## true_class cats dogs
##       cats  303  170
##       dogs  655 1601
```

```r
report_accuracy_and_auc(true_class, pred_glm)
```

```
## AUC: 0.736
## ACC: 0.698
## Cats: Recall: 0.641
##       Precision: 0.316
## Dogs: Recall: 0.71
##       Precision: 0.904
```

But this is almost equivalent to up sampling.

A more intelligent use of class weights would be something like using the `class.weights` parameter in `e1071::svm()`

---

### Cost aware training: Prior probabilities

**Small Detour: Naive Bayes**

You know Bayes' Theorem, right?

`\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)`

or

`\(posterior = \frac{likelihood \cdot prior}{evidence}\)`

So what would be the posterior probability of class `\(C_k\)` given that we've seen observation `\(x_i\)`?

`\(P(C_k|x_i) = \frac{P(x_i|C_k)P(C_k)}{P(x_i)}\)`

---

`\(P(C_k|x_i) = \frac{P(x_i|C_k)P(C_k)}{P(x_i)}\)`

In words: the likelihood of seeing an observation like `\(x_i\)` in all class `\(C_k\)` observations, times the prior of class `\(C_k\)` observations, divided by the evidence seeing an observation like `\(x_i\)` in general.

.insight[
üí° What increases `\(P(C_k|x_i)\)`? What decreases it?
]

But if we have, say 100 predictors, each categorical with 2 levels - we'd have to pre-compute `\(2^{100}\)` possible for each `\(C_k\)`!

---

Enter *Naive* Bayes:

Assume that all predictors `\(X\)`  are mutually independent, conditional on the class `\(C_k\)`, and so:

`\(P(x_i|C_k) = \prod_{j = 1}^pP(x_{ij}|C_k)\)`

And so:

`\(P(C_k|x_i) = \frac{\prod P(x_{ij}|C_k)P(C_k)}{P(x_i)}\)`

And we can further expand: `\(P(x_i) = \sum_k P(x_{i}|C_k)P(C_k)\)`

.insight[
üí° How would you compute `\(P(x_{ij}|C_k)\)` when `\(x_{ij}\)` is continuous?
]

---


```r
library(naivebayes)

mod_nb &lt;- naive_bayes(pets ~ ., data = ok_train)

pred_nb &lt;- predict(mod_nb, ok_valid, type = "prob")[, "cats"]

pred_nb_class &lt;- ifelse(pred_nb &gt; 0.5, "cats", "dogs")
table(true_class, pred_nb_class)
```

```
##           pred_nb_class
## true_class cats dogs
##       cats  180  293
##       dogs  294 1962
```

```r
report_accuracy_and_auc(true_class, pred_nb)
```

```
## AUC: 0.718
## ACC: 0.785
## Cats: Recall: 0.381
##       Precision: 0.38
## Dogs: Recall: 0.87
##       Precision: 0.87
```

---

BTW, *are* our features mutually independent?


```r
ok_train %&gt;%
  filter(pets == "cats") %&gt;%
  select_if(is.numeric) %&gt;%
  cor() %&gt;%
  corrplot::corrplot()
```

&lt;img src="images/Imbalanced-Cor-Matrix-1.png" width="50%" /&gt;

---

In the context of imbalanced classes you could just give a 5 times more weight to the score of cats by specifying different prior probabilities `\(P(C_k)\)`:


```r
mod_nb &lt;- naive_bayes(pets ~ ., data = ok_train, prior = c(5, 1))

pred_nb &lt;- predict(mod_nb, ok_valid, type = "prob")[, "cats"]

pred_nb_class &lt;- ifelse(pred_nb &gt; 0.5, "cats", "dogs")
table(true_class, pred_nb_class)
```

```
##           pred_nb_class
## true_class cats dogs
##       cats  398   75
##       dogs 1251 1005
```

```r
report_accuracy_and_auc(true_class, pred_nb)
```

```
## AUC: 0.718
## ACC: 0.514
## Cats: Recall: 0.841
##       Precision: 0.241
## Dogs: Recall: 0.445
##       Precision: 0.931
```

---

### Down Sampling

Yes, down sampling the majority class, usually to make it the same amount as the minority class (but you can tune this parameter as any other).

You'd be surprised.


```r
rec_gbt &lt;- recipe(pets ~ ., data = ok_train) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
* themis::step_downsample(pets, under_ratio = 1) %&gt;%
  prep(ok_train)
```

If you want to stay in the `tidymodels` framework you can download the `themis` package for extra recipes for dealing with unbalanced data.

---

.warning[
‚ö†Ô∏è Never down-sample the testing set! Look at the `skip` parameter.
]


```r
juice(rec_gbt) %&gt;% count(pets)
```

```
## # A tibble: 2 x 2
##   pets      n
##   &lt;fct&gt; &lt;int&gt;
## 1 cats   1624
## 2 dogs   1624
```

```r
bake(rec_gbt, ok_valid) %&gt;% count(pets)
```

```
## # A tibble: 2 x 2
##   pets      n
##   &lt;fct&gt; &lt;int&gt;
## 1 cats    473
## 2 dogs   2256
```

---


```r
mod_gbt &lt;- mod_gbt_spec %&gt;%
  fit(pets ~ ., data = juice(rec_gbt))
pred_gbt &lt;- mod_gbt %&gt;%
  predict(new_data = bake(rec_gbt, ok_valid), type = "prob") %&gt;%
  pull(.pred_cats)
pred_gbt_class &lt;- ifelse(pred_gbt &gt; 0.5, "cats", "dogs")

table(true_class, pred_gbt_class)
```

```
##           pred_gbt_class
## true_class cats dogs
##       cats  335  138
##       dogs  733 1523
```

```r
report_accuracy_and_auc(true_class, pred_gbt)
```

```
## AUC: 0.749
## ACC: 0.681
## Cats: Recall: 0.708
##       Precision: 0.314
## Dogs: Recall: 0.675
##       Precision: 0.917
```

---

### Up Sampling

The main disadvantage of down sampling is of course the loss of data.

Will replicating (minority class) data do any better?


```r
rec_gbt &lt;- recipe(pets ~ ., data = ok_train) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
* themis::step_upsample(pets, over_ratio = 1) %&gt;%
  prep(ok_train)

mod_gbt &lt;- mod_gbt_spec %&gt;%
  fit(pets ~ ., data = juice(rec_gbt))
pred_gbt &lt;- mod_gbt %&gt;%
  predict(new_data = bake(rec_gbt, ok_valid), type = "prob") %&gt;%
  pull(.pred_cats)
pred_gbt_class &lt;- ifelse(pred_gbt &gt; 0.5, "cats", "dogs")
```

---


```r
table(true_class, pred_gbt_class)
```

```
##           pred_gbt_class
## true_class cats dogs
##       cats  263  210
##       dogs  499 1757
```

```r
report_accuracy_and_auc(true_class, pred_gbt)
```

```
## AUC: 0.739
## ACC: 0.74
## Cats: Recall: 0.556
##       Precision: 0.345
## Dogs: Recall: 0.779
##       Precision: 0.893
```

---

### SMOTE

[Chawla et. al. (2002)](https://arxiv.org/pdf/1106.1813.pdf) developed SMOTE (Synthetic Minority Over-sampling Technique) which is a up sampling technique.

The authors claim that a hybrid combination of SMOTE and regular down sampling works best, that's why SMOTE is sometimes referred to as a "hybrid" sampling algo itself.

But the up sampling does not simply replicates the minority class...

---

It synthesizes them!

&lt;img src="images/smote_algo.png" style="width: 70%" /&gt;

---


```r
n &lt;- 100
x1 &lt;- rnorm(n, 0, 1); x2 &lt;- rnorm(n, 0, 1)
t &lt;- 2 - 4 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))

df &lt;- tibble(x1 = x1, x2 = x2, y = factor(y))
df %&gt;% count(y)
```

```
## # A tibble: 2 x 2
##   y         n
##   &lt;fct&gt; &lt;int&gt;
## 1 0        29
## 2 1        71
```

```r
df_smoted &lt;- recipe(y ~ ., data = df) %&gt;%
  themis::step_smote(y, over_ratio = 1) %&gt;%
  prep(df) %&gt;%
  juice()
```

---


```r
df_smoted %&gt;% count(y)
```

```
## # A tibble: 2 x 2
##   y         n
##   &lt;fct&gt; &lt;int&gt;
## 1 0        71
## 2 1        71
```

&lt;img src="images/SMOTE-simulation-1.png" width="100%" /&gt;

---

Let's do a hybrid of down sampling and SMOTE on our data:


```r
rec_gbt &lt;- recipe(pets ~ ., data = ok_train) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
  themis::step_downsample(pets, under_ratio = 1.5) %&gt;%
  themis::step_smote(pets, over_ratio = 1) %&gt;%
  prep(ok_train)

juice(rec_gbt) %&gt;% count(pets)
```

```
## # A tibble: 2 x 2
##   pets      n
##   &lt;fct&gt; &lt;int&gt;
## 1 cats   2436
## 2 dogs   2436
```

```r
mod_gbt &lt;- mod_gbt_spec %&gt;%
  fit(pets ~ ., data = juice(rec_gbt))
pred_gbt &lt;- mod_gbt %&gt;%
  predict(new_data = bake(rec_gbt, ok_valid), type = "prob") %&gt;%
  pull(.pred_cats)
pred_gbt_class &lt;- ifelse(pred_gbt &gt; 0.5, "cats", "dogs")
```

---


```r
table(true_class, pred_gbt_class)
```

```
##           pred_gbt_class
## true_class cats dogs
##       cats  274  199
##       dogs  492 1764
```

```r
report_accuracy_and_auc(true_class, pred_gbt)
```

```
## AUC: 0.745
## ACC: 0.747
## Cats: Recall: 0.579
##       Precision: 0.358
## Dogs: Recall: 0.782
##       Precision: 0.899
```

---

### Other Up Sampling Methods

- ADASYN
- Borderline SMOTE
- ROSE
- Depends on data (e.g. with images it is common to perform image augmentation: flip, crop, rotate, blur it)

### Change of Framework

- Anomaly Detection

---

### One final word of wisdom

- It's Ok for a model to not know!

- The optimal choice would be for a classification model to output a *score*, rather than a class, and have the client's system interpret that score for different applications

- However, if a class output is required, consider outputting a `\(k+1\)` class: "I don't know"

- In the case of classifying cats vs. dogs people - it makes sense!

- For a two-class problem, you would have not one cutoff on the score, but two: Below cutoff 1 classify as "Dogs", above "Cats" and in the middle: "I don't know"

- As long as you make a decision regarding at least X% of the data

---

Let's see this on the `ok_valid` test set using our last SMOTEd model (but notice to tune these cutoffs you would need an extra set of untouched data!):


```r
upper &lt;- 0.65
lower &lt;- 0.35
pred_gbt_class &lt;- ifelse(pred_gbt &gt; upper, "cats",
                         ifelse(pred_gbt &lt; lower, "dogs", NA))
table(true_class, pred_gbt_class)
```

```
##           pred_gbt_class
## true_class cats dogs
##       cats  153  115
##       dogs  231 1378
```

---


```r
report_accuracy_and_auc2 &lt;- function(obs, pred, lower = 0.35, upper = 0.65) {
  pred_class &lt;- ifelse(pred &gt; upper, "cats",
                         ifelse(pred &lt; lower, "dogs", NA))
  cm &lt;- table(true_class, pred_class)
  recall_cats &lt;- cm[1, 1] / sum(cm[1,])
  recall_dogs &lt;- cm[2, 2] / sum(cm[2,])
  prec_cats &lt;- cm[1, 1] / sum(cm[,1])
  prec_dogs &lt;- cm[2, 2] / sum(cm[,2])
  acc &lt;- sum(diag(cm)) / sum(cm)
  pred_pct &lt;- sum(cm) / length(obs)
  glue::glue("Predicted: {format(pred_pct, digits = 3)}
  ACC: {format(acc, digits = 3)}
  Cats: Recall: {format(recall_cats, digits = 3)}
        Precision: {format(prec_cats, digits = 3)}
  Dogs: Recall: {format(recall_dogs, digits = 3)}
        Precision: {format(prec_dogs, digits = 3)}")
}

report_accuracy_and_auc2(true_class, pred_gbt)
```

```
## Predicted: 0.688
## ACC: 0.816
## Cats: Recall: 0.571
##       Precision: 0.398
## Dogs: Recall: 0.856
##       Precision: 0.923
```

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
