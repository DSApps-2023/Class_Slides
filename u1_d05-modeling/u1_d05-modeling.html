<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Modeling in the Tidyverse</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2021-02-09" />
    <script src="u1_d05-modeling_files/header-attrs-2.6/header-attrs.js"></script>
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Modeling in the Tidyverse

### Applications of Data Science - Class 5

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2021-02-09

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2021.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# The Problem

---

### Inconsistency, Inextensibility


```r
n &lt;- 10000
x1 &lt;- runif(n)
x2 &lt;- runif(n)
t &lt;- 1 + 2 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))
```


```r
glm(y ~ x1 + x2, family = "binomial")
```


```r
glmnet(as.matrix(cbind(x1, x2)), as.factor(y), family = "binomial")
```


```r
randomForest(as.factor(y) ~ x1 + x2)
```



```r
gbm(y ~ x1 + x2, data = data.frame(x1 = x1, x2 = x2, y = y))
```

üò±

---

### Compare this with `sklearn`


```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier,
  GradientBoostingClassifier

LogisticRegression(penalty='none').fit(X, y)

LogisticRegression(penalty='l2', C=0.001).fit(X, y)

RandomForestClassifier(n_estimators=100).fit(X, y)

GradientBoostingClassifier(n_estimators=100).fit(X, y)
```

---

class: section-slide

# Detour: A Regression Problem

---

### IPF-Lifts: Predicting Bench Lifting

- Dataset was published as part of the [TidyTuesday](https://github.com/rfordatascience/tidytuesday) intiative
- Comes from [Open Powerlifting](https://www.openpowerlifting.org/data)
- [Wikipedia](https://en.wikipedia.org/wiki/Powerlifting): Powerlifting is a strength sport that consists of three attempts at maximal weight on three lifts: squat, bench press, and deadlift

&lt;img src="images/pl_bench.jpg" style="width: 70%" /&gt;

---

The raw data has over 40K rows: for each athlete, for each event, stats about athlete gender, age and weight, and the maximal weight lifted in the 3 types of Powerlifting.

We will be predicting `best3bench_kg` based on a few predictors, no missing values:


```r
library(lubridate)

ipf_lifts &lt;- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-08/ipf_lifts.csv")

ipf_lifts &lt;- ipf_lifts %&gt;%
  drop_na(best3bench_kg, age) %&gt;%
  filter(between(age, 18, 100), best3bench_kg &gt; 0, equipment != "Wraps") %&gt;%
  select(sex, event, equipment, age, division, bodyweight_kg, best3bench_kg, date, meet_name) %&gt;%
  drop_na() %&gt;%
  mutate(year = year(date), month = month(date),
         dayofweek = wday(date)) %&gt;%
  select(-date) %&gt;%
  mutate(across(where(is.character), as.factor))

dim(ipf_lifts)
```

```
## [1] 32047    11
```

---


```r
glimpse(ipf_lifts)
```

```
## Rows: 32,047
## Columns: 11
## $ sex           &lt;fct&gt; F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F,...
## $ event         &lt;fct&gt; SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD, SBD...
## $ equipment     &lt;fct&gt; Single-ply, Single-ply, Single-ply, Single-ply, Singl...
## $ age           &lt;dbl&gt; 33.5, 34.5, 23.5, 27.5, 37.5, 25.5, 33.5, 26.0, 33.5,...
## $ division      &lt;fct&gt; Open, Open, Open, Open, Open, Open, Open, Open, Open,...
## $ bodyweight_kg &lt;dbl&gt; 44, 44, 44, 44, 44, 44, 48, 48, 48, 48, 48, 48, 48, 5...
## $ best3bench_kg &lt;dbl&gt; 60.0, 62.5, 62.5, 60.0, 65.0, 45.0, 62.5, 77.5, 65.0,...
## $ meet_name     &lt;fct&gt; World Powerlifting Championships, World Powerlifting ...
## $ year          &lt;dbl&gt; 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989, 1989,...
## $ month         &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1...
## $ dayofweek     &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...
```

---

See the dependent variable distribution:


```r
ggplot(ipf_lifts, aes(best3bench_kg)) +
  geom_histogram(fill = "red", alpha = 0.5) +
  theme_classic()
```

&lt;img src="images/Bench-Hist-1.png" width="100%" /&gt;

---

See it vs. say age, facetted by equipment:


```r
ggplot(ipf_lifts, aes(age, best3bench_kg)) +
  geom_point(color = "red", alpha = 0.5) +
  facet_wrap(~ equipment) +
  theme_classic()
```

&lt;img src="images/Bench-Age-Equipment-1.png" width="100%" /&gt;

---

See it vs. year, by gender:


```r
ggplot(ipf_lifts, aes(factor(year), best3bench_kg, fill = sex)) +
  geom_boxplot(outlier.alpha = 0.5) +
  labs(fill = "", x = "", y = "") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

&lt;img src="images/Bench-Year-Gender-1.png" width="100%" /&gt;

---

Maybe add `\(age^2\)` and `\(year^2\)` to make Linear Regression's life easier?


```r
ipf_lifts &lt;- ipf_lifts %&gt;%
  mutate(age2 = age ^ 2, year2 = year ^2)
```

---

class: section-slide

# End of Detour

---

# WARNING

.warning[
üí° What you're about to see is not a good modeling/prediction flow. This is just an intro to tidy modeling. Some of the issues with how things are done here will be raised, some will have to wait till later in the course.
]
---

class: section-slide

# The Present Solution: `caret`

---

### Split Data


```r
library(caret)

train_idx &lt;- createDataPartition(ipf_lifts$best3bench_kg,
                                 p = 0.6, list = FALSE)

ipf_tr &lt;- ipf_lifts[train_idx, ]
ipf_te &lt;- ipf_lifts[-train_idx, ]

library(glue)
glue("train no. of rows: {nrow(ipf_tr)}
     test no. of rows: {nrow(ipf_te)}")
```

```
## train no. of rows: 19230
## test no. of rows: 12817
```

Here you might consider some preprocessing.

`caret` has some nice documentation [here](https://topepo.github.io/caret/index.html).

---

### Tuning and Modeling

Define general methodology, e.g. 5-fold Cross-Validation:


```r
fit_control &lt;- trainControl(method = "cv", number = 5)

ridge_grid &lt;- expand.grid(alpha=0, lambda = 10^seq(-3, 1, length = 50))
lasso_grid &lt;- expand.grid(alpha=1, lambda = 10^seq(-3, 1, length = 50))
rf_grid &lt;- expand.grid(splitrule = "variance",
                       min.node.size = seq(10, 30, 10),
                       mtry = seq(2, 10, 2))

mod_ridge &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = ridge_grid,
                metric = "RMSE")

mod_lasso &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = lasso_grid,
                metric = "RMSE")

mod_rf &lt;- train(best3bench_kg ~ ., data = ipf_tr, method = "ranger",
                trControl = fit_control, tuneGrid = rf_grid,
                num.trees = 50, metric = "RMSE")
```

---

### Evaluating Models


```r
mod_ridge
```

```
## glmnet 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15383, 15385, 15383, 15384, 15385 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE      Rsquared   MAE     
##    0.001000000  26.63908  0.8135187  20.46183
##    0.001206793  26.63908  0.8135187  20.46183
##    0.001456348  26.63908  0.8135187  20.46183
##    0.001757511  26.63908  0.8135187  20.46183
##    0.002120951  26.63908  0.8135187  20.46183
##    0.002559548  26.63908  0.8135187  20.46183
##    0.003088844  26.63908  0.8135187  20.46183
##    0.003727594  26.63908  0.8135187  20.46183
##    0.004498433  26.63908  0.8135187  20.46183
##    0.005428675  26.63908  0.8135187  20.46183
##    0.006551286  26.63908  0.8135187  20.46183
##    0.007906043  26.63908  0.8135187  20.46183
##    0.009540955  26.63908  0.8135187  20.46183
##    0.011513954  26.63908  0.8135187  20.46183
##    0.013894955  26.63908  0.8135187  20.46183
##    0.016768329  26.63908  0.8135187  20.46183
##    0.020235896  26.63908  0.8135187  20.46183
##    0.024420531  26.63908  0.8135187  20.46183
##    0.029470517  26.63908  0.8135187  20.46183
##    0.035564803  26.63908  0.8135187  20.46183
##    0.042919343  26.63908  0.8135187  20.46183
##    0.051794747  26.63908  0.8135187  20.46183
##    0.062505519  26.63908  0.8135187  20.46183
##    0.075431201  26.63908  0.8135187  20.46183
##    0.091029818  26.63908  0.8135187  20.46183
##    0.109854114  26.63908  0.8135187  20.46183
##    0.132571137  26.63908  0.8135187  20.46183
##    0.159985872  26.63908  0.8135187  20.46183
##    0.193069773  26.63908  0.8135187  20.46183
##    0.232995181  26.63908  0.8135187  20.46183
##    0.281176870  26.63908  0.8135187  20.46183
##    0.339322177  26.63908  0.8135187  20.46183
##    0.409491506  26.63908  0.8135187  20.46183
##    0.494171336  26.63908  0.8135187  20.46183
##    0.596362332  26.63908  0.8135187  20.46183
##    0.719685673  26.63908  0.8135187  20.46183
##    0.868511374  26.63908  0.8135187  20.46183
##    1.048113134  26.63908  0.8135187  20.46183
##    1.264855217  26.63908  0.8135187  20.46183
##    1.526417967  26.63908  0.8135187  20.46183
##    1.842069969  26.63908  0.8135187  20.46183
##    2.222996483  26.63908  0.8135187  20.46183
##    2.682695795  26.63908  0.8135187  20.46183
##    3.237457543  26.63908  0.8135187  20.46183
##    3.906939937  26.63908  0.8135187  20.46183
##    4.714866363  26.67656  0.8133118  20.48961
##    5.689866029  26.77458  0.8128159  20.56351
##    6.866488450  26.90058  0.8122599  20.66091
##    8.286427729  27.06407  0.8116232  20.78744
##   10.000000000  27.27573  0.8108998  20.95436
## 
## Tuning parameter 'alpha' was held constant at a value of 0
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 3.90694.
```

---


```r
mod_lasso
```

```
## glmnet 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15384, 15384, 15383, 15384, 15385 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE      Rsquared   MAE     
##    0.001000000  26.17362  0.8184162  20.11682
##    0.001206793  26.17362  0.8184162  20.11682
##    0.001456348  26.17362  0.8184162  20.11682
##    0.001757511  26.17362  0.8184162  20.11682
##    0.002120951  26.17362  0.8184162  20.11682
##    0.002559548  26.17362  0.8184162  20.11682
##    0.003088844  26.17362  0.8184162  20.11682
##    0.003727594  26.17362  0.8184162  20.11682
##    0.004498433  26.17362  0.8184162  20.11682
##    0.005428675  26.17362  0.8184162  20.11682
##    0.006551286  26.17362  0.8184162  20.11682
##    0.007906043  26.17362  0.8184162  20.11682
##    0.009540955  26.17362  0.8184162  20.11682
##    0.011513954  26.17362  0.8184162  20.11682
##    0.013894955  26.17362  0.8184162  20.11682
##    0.016768329  26.17362  0.8184162  20.11682
##    0.020235896  26.17367  0.8184157  20.11689
##    0.024420531  26.17664  0.8183758  20.11966
##    0.029470517  26.18489  0.8182621  20.12683
##    0.035564803  26.19425  0.8181338  20.13412
##    0.042919343  26.20581  0.8179748  20.14283
##    0.051794747  26.21714  0.8178195  20.15139
##    0.062505519  26.23099  0.8176296  20.16186
##    0.075431201  26.25609  0.8172829  20.18101
##    0.091029818  26.28348  0.8169065  20.20339
##    0.109854114  26.31377  0.8164914  20.22648
##    0.132571137  26.35223  0.8159631  20.25502
##    0.159985872  26.40770  0.8151969  20.29513
##    0.193069773  26.44259  0.8147249  20.31812
##    0.232995181  26.45543  0.8145744  20.32409
##    0.281176870  26.47050  0.8144058  20.33132
##    0.339322177  26.48954  0.8141989  20.34152
##    0.409491506  26.51496  0.8139288  20.35610
##    0.494171336  26.55110  0.8135408  20.37760
##    0.596362332  26.60248  0.8129846  20.40978
##    0.719685673  26.67385  0.8122111  20.45397
##    0.868511374  26.76456  0.8112501  20.51050
##    1.048113134  26.87113  0.8101914  20.58003
##    1.264855217  27.02038  0.8086898  20.68219
##    1.526417967  27.23278  0.8064910  20.83197
##    1.842069969  27.51006  0.8036387  21.02563
##    2.222996483  27.87091  0.7999362  21.27629
##    2.682695795  28.35290  0.7948636  21.61192
##    3.237457543  28.99738  0.7877939  22.08062
##    3.906939937  29.83882  0.7781041  22.72040
##    4.714866363  30.87683  0.7657136  23.55373
##    5.689866029  32.30205  0.7464616  24.73956
##    6.866488450  33.87456  0.7243770  26.07656
##    8.286427729  35.33094  0.7060128  27.34149
##   10.000000000  37.21302  0.6787253  28.94034
## 
## Tuning parameter 'alpha' was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.01676833.
```

---


```r
mod_rf
```

```
## Random Forest 
## 
## 19230 samples
##    12 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 15383, 15384, 15383, 15385, 15385 
## Resampling results across tuning parameters:
## 
##   min.node.size  mtry  RMSE      Rsquared   MAE     
##   10              2    43.98567  0.7149522  35.33115
##   10              4    33.76349  0.7849199  26.46574
##   10              6    28.13475  0.8242156  21.82625
##   10              8    25.82057  0.8380935  19.86416
##   10             10    24.36200  0.8466544  18.67136
##   20              2    43.82008  0.7278177  35.21164
##   20              4    33.57668  0.7903280  26.39480
##   20              6    28.34047  0.8207030  21.87692
##   20              8    25.66929  0.8384042  19.79244
##   20             10    24.43080  0.8465645  18.74618
##   30              2    43.79985  0.7148687  35.15808
##   30              4    33.89321  0.7833205  26.55198
##   30              6    28.60901  0.8191121  22.18501
##   30              8    26.15951  0.8342340  20.06639
##   30             10    24.48731  0.8460840  18.75604
## 
## Tuning parameter 'splitrule' was held constant at a value of variance
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 10, splitrule = variance
##  and min.node.size = 10.
```

---


```r
plot(mod_ridge)
```

&lt;img src="images/Ridge-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_lasso)
```

&lt;img src="images/Lasso-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_rf)
```

&lt;img src="images/RF-CV-1.png" width="80%" /&gt;

---

### Comparing Models


```r
resamps &lt;- resamples(list(Ridge = mod_ridge,
                          Lasso = mod_lasso,
                          RF = mod_rf))
summary(resamps)
```

```
## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: Ridge, Lasso, RF 
## Number of resamples: 5 
## 
## MAE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## Ridge 20.14839 20.29589 20.46453 20.46183 20.65557 20.74478    0
## Lasso 19.94215 20.01630 20.09051 20.11682 20.18540 20.34974    0
## RF    18.35277 18.48495 18.53554 18.67136 18.93460 19.04897    0
## 
## RMSE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## Ridge 26.24309 26.42769 26.70235 26.63908 26.88262 26.93966    0
## Lasso 25.99027 26.04725 26.11301 26.17362 26.34091 26.37665    0
## RF    24.00869 24.06167 24.19071 24.36200 24.47448 25.07447    0
## 
## Rsquared 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.8086262 0.8087582 0.8157508 0.8135187 0.8160303 0.8184279    0
## Lasso 0.8172126 0.8175876 0.8182555 0.8184162 0.8189501 0.8200754    0
## RF    0.8362815 0.8462720 0.8479404 0.8466544 0.8492569 0.8535211    0
```

---


```r
dotplot(resamps, metric = "RMSE")
```

&lt;img src="images/Caret-RMSE-Comp-1.png" width="100%" /&gt;

---

### Predicting


```r
pred_ridge &lt;- predict(mod_ridge, newdata = ipf_te)
pred_lasso &lt;- predict(mod_lasso, newdata = ipf_te)
pred_rf &lt;- predict(mod_rf, newdata = ipf_te)

rmse_ridge &lt;- RMSE(pred_ridge, ipf_te$best3bench_kg)
rmse_lasso &lt;- RMSE(pred_lasso, ipf_te$best3bench_kg)
rmse_rf &lt;- RMSE(pred_rf, ipf_te$best3bench_kg)

glue("Test RMSE Ridge: {format(rmse_ridge, digits = 3)}
     Test RMSE Lassoe: {format(rmse_lasso, digits = 3)}
     Test RMSE RF: {format(rmse_rf, digits = 3)}")
```

```
## Test RMSE Ridge: 26.7
## Test RMSE Lassoe: 26.4
## Test RMSE RF: 24.7
```

.warning[
‚ö†Ô∏è Is using a "regular" regression model the natural approach for these data?

Ask yourself what is this model good for, if at all ü§≠
]

---


```r
bind_rows(
  tibble(method = "Ridge", pred = pred_ridge, true = ipf_te$best3bench_kg),
  tibble(method = "Lasso", pred = pred_lasso, true = ipf_te$best3bench_kg),
  tibble(method = "RF", pred = pred_rf, true = ipf_te$best3bench_kg)) %&gt;%
  ggplot(aes(pred, true)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ method) +
  theme_bw()
```

&lt;img src="images/Caret-Pred-vs-True-1.png" width="100%" /&gt;

---

class: section-slide

# The Future Solution: `tidymodels`

#### Inspired by [Julia Silge](https://juliasilge.com/blog/intro-tidymodels/)

---

### Packages under tidymodels

- `parsnip`: tidy `caret`
- `dials` and `tune`: specifying and tuning model parameters
- `rsample`: sampling, data partitioning
- `recipes`, `embed`, `themis`: preprocessing and creating model matrices
- `infer`: tidy statistics
- `yardstick`: measuring models performance
- `broom`: convert models output into tidy tibbles

And [more](https://www.tidymodels.org/).

.warning[
‚ö†Ô∏è All `tidymodels` packages are under development!
]

---

### Split Data

The `initial_split()` function is from the `rsample` package:


```r
library(tidymodels)

ipf_split_obj &lt;- ipf_lifts %&gt;%
  initial_split(prop = 0.6, strata = equipment)

ipf_tr &lt;- training(ipf_split_obj)
ipf_te &lt;- testing(ipf_split_obj)

glue("train no. of rows: {nrow(ipf_tr)}
     test no. of rows: {nrow(ipf_te)}")
```

```
## train no. of rows: 19229
## test no. of rows: 12818
```

```r
print(ipf_split_obj)
```

```
## &lt;Analysis/Assess/Total&gt;
## &lt;19229/12818/32047&gt;
```

---

### Preprocess .font80percent[(but we're not gonna use it)]

The `recipe()` function is from the `recipes` package. It allows you to specify a python-like pipe you can later apply to any dataset, including all preprocessing steps:


```r
ipf_rec &lt;- recipe(best3bench_kg ~ ., data = ipf_tr)
ipf_rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         12
```

`recipes` contains more preprocessing [`step_`s](https://tidymodels.github.io/recipes/reference/index.html) than you imagine:


```r
ipf_rec &lt;-  ipf_rec %&gt;%
  step_normalize(all_numeric())
```

---

After you have your `recipe` you need to `prep()` materials...


```r
ipf_rec &lt;- ipf_rec %&gt;% prep(ipf_tr)

ipf_rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         12
## 
## Training data contained 19229 data points and no missing data.
## 
## Operations:
## 
## Centering and scaling for age, bodyweight_kg, year, month, ... [trained]
```

At this point our `recipe` has all necessary `sd` and `mean`s for numeric variables.

---


```r
ipf_rec$var_info
```

```
## # A tibble: 13 x 4
##    variable      type    role      source  
##    &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
##  1 sex           nominal predictor original
##  2 event         nominal predictor original
##  3 equipment     nominal predictor original
##  4 age           numeric predictor original
##  5 division      nominal predictor original
##  6 bodyweight_kg numeric predictor original
##  7 meet_name     nominal predictor original
##  8 year          numeric predictor original
##  9 month         numeric predictor original
## 10 dayofweek     numeric predictor original
## 11 age2          numeric predictor original
## 12 year2         numeric predictor original
## 13 best3bench_kg numeric outcome   original
```

---


```r
ipf_rec$levels$meet_name
```

```
## $values
##  [1] "3rd World University Powerlifting Cup"                 
##  [2] "48th World Open Championships"                         
##  [3] "6th World Classic Championships"                       
##  [4] "Disabled Bench Press World Championships"              
##  [5] "Men's World Powerlifting Championships"                
##  [6] "Reykjav√≠k International Games"                         
##  [7] "Student's World Cup"                                   
##  [8] "University Powerlifting Cup"                           
##  [9] "Women's World Powerlifting Championship"               
## [10] "Women's World Powerlifting Championships"              
## [11] "World Bench Press Championships"                       
## [12] "World Classic Bench Press Championships"               
## [13] "World Classic Powerlifting Championships"              
## [14] "World Classic Powerlifting Cup"                        
## [15] "World Disabled Bench Press Championships"              
## [16] "World Games"                                           
## [17] "World Juniors &amp; Sub-Juniors Championships"             
## [18] "World Juniors Powerlifting Championships"              
## [19] "World Masters Bench Press Championships"               
## [20] "World Masters Championships"                           
## [21] "World Masters Powerlifting Championships"              
## [22] "World Men's Powerlifting Championship"                 
## [23] "World Open Bench Press Championships"                  
## [24] "World Open Championships"                              
## [25] "World Powerlifting Championships"                      
## [26] "World Students Cup"                                    
## [27] "World Sub-Juniors &amp; Juniors Bench Press Championships" 
## [28] "World Sub-Juniors &amp; Juniors Powerlifting Championships"
## [29] "World Sub-Juniors Powerlifting Championships"          
## [30] "World University Powerlifting Cup"                     
## 
## $ordered
## [1] FALSE
## 
## $factor
## [1] TRUE
```

---


```r
ipf_rec$steps[[1]]$means
```

```
##           age bodyweight_kg          year         month     dayofweek 
##  3.657200e+01  8.156813e+01  2.006920e+03  7.749597e+00  3.587446e+00 
##          age2         year2 best3bench_kg 
##  1.539900e+03  4.027817e+06  1.481113e+02
```

```r
ipf_rec$steps[[1]]$sds
```

```
##           age bodyweight_kg          year         month     dayofweek 
##     14.226722     25.066822      9.375594      2.724269      1.745484 
##          age2         year2 best3bench_kg 
##   1186.398547  37579.912084     61.057414
```

---

And then we `bake()` (or [`juice()`](https://tidymodels.github.io/recipes/reference/juice.html)):


```r
ipf_tr2 &lt;- ipf_rec %&gt;% bake(ipf_tr)
ipf_te2 &lt;- ipf_rec %&gt;% bake(ipf_te)

glue("mean of age in orig training: {format(mean(ipf_tr$age), digits = 3)}, sd: {format(sd(ipf_tr$age), digits = 3)}
     mean of age in baked training: {format(mean(ipf_tr2$age), digits = 0)}, sd: {format(sd(ipf_tr2$age), digits = 3)}")
```

```
## mean of age in orig training: 36.6, sd: 14.2
## mean of age in baked training: -0, sd: 1
```

```r
glue("mean of age in orig testing: {format(mean(ipf_te$age), digits = 3)}, sd: {format(sd(ipf_te$age), digits = 3)}
     mean of age in baked testing: {format(mean(ipf_te2$age), digits = 0)}, sd: {format(sd(ipf_te2$age), digits = 3)}")
```

```
## mean of age in orig testing: 36.7, sd: 14.4
## mean of age in baked testing: 0, sd: 1.01
```

---

Or you can do it all in a single pipe:


```r
ipf_rec &lt;- recipe(best3bench_kg ~ ., data = ipf_tr) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  prep(ipf_tr)

ipf_tr2 &lt;- ipf_rec %&gt;% bake(ipf_tr)
ipf_te2 &lt;- ipf_rec %&gt;% bake(ipf_te)

glue("mean of age in orig training: {format(mean(ipf_tr$age), digits = 3)}, sd: {format(sd(ipf_tr$age), digits = 3)}
     mean of age in baked training: {format(mean(ipf_tr2$age), digits = 0)}, sd: {format(sd(ipf_tr2$age), digits = 2)}")
```

```
## mean of age in orig training: 36.6, sd: 14.2
## mean of age in baked training: -0, sd: 1
```

```r
glue("mean of age in orig testing: {format(mean(ipf_te$age), digits = 3)}, sd: {format(sd(ipf_te$age), digits = 3)}
     mean of age in baked testing: {format(mean(ipf_te2$age), digits = 0)}, sd: {format(sd(ipf_te2$age), digits = 3)}")
```

```
## mean of age in orig testing: 36.7, sd: 14.4
## mean of age in baked testing: 0, sd: 1.01
```

---

#### Fast Forward 10 weeks from now...


```r
rec_int_topints &lt;- recipe(pets ~ ., data = okcupid_tr) %&gt;%
  step_textfeature(essays, prefix = "t",
                   extract_functions = my_text_funs) %&gt;%
  update_role(essays, new_role = "discarded") %&gt;%
  step_mutate_at(starts_with("t_"), fn = ~ifelse(is.na(.x), 0, .x)) %&gt;%
  step_log(income, starts_with("len_"), starts_with("t_"),
           -t_essays_sent_bing, offset = 1) %&gt;%
  step_meanimpute(income) %&gt;%
  step_other(
    all_nominal(), -has_role("discarded"), -all_outcomes(),
    other = "all_else", threshold = 0.1) %&gt;%
  step_novel(
    all_nominal(), -has_role("discarded"), -all_outcomes()) %&gt;%
  step_modeimpute(all_nominal(), -has_role("discarded"), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(),
             -has_role("discarded"), one_hot = FALSE) %&gt;%
  step_interact(topint_ints) %&gt;%
  step_nzv(all_numeric(), freq_cut = 99/1) %&gt;%
  step_upsample(pets, over_ratio = 1, seed = 42)
```

---

### Modeling

For now let us use the original `ipf_tr` data.

Functions `linear_reg()` and `set_engine()` are from the `parsnip` package:


```r
mod_ridge_spec &lt;- linear_reg(mixture = 0, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet")

mod_ridge_spec
```

```
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0.001
##   mixture = 0
## 
## Computational engine: glmnet
```

---


```r
mod_ridge &lt;- mod_ridge_spec %&gt;%
  fit(best3bench_kg ~ ., data = ipf_tr)

mod_ridge
```

```
## parsnip model object
## 
## Fit time:  60ms 
## 
## Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = "gaussian",      alpha = ~0) 
## 
##     Df  %Dev Lambda
## 1   51  0.00  43280
## 2   51  0.39  39440
## 3   51  0.43  35940
## 4   51  0.47  32740
## 5   51  0.52  29830
## 6   51  0.57  27180
## 7   51  0.62  24770
## 8   51  0.68  22570
## 9   51  0.75  20560
## 10  51  0.82  18740
## 11  51  0.90  17070
## 12  51  0.99  15560
## 13  51  1.08  14170
## 14  51  1.19  12910
## 15  51  1.30  11770
## 16  51  1.43  10720
## 17  51  1.56   9769
## 18  51  1.71   8902
## 19  51  1.88   8111
## 20  51  2.05   7390
## 21  51  2.25   6734
## 22  51  2.46   6136
## 23  51  2.70   5590
## 24  51  2.95   5094
## 25  51  3.23   4641
## 26  51  3.53   4229
## 27  51  3.87   3853
## 28  51  4.23   3511
## 29  51  4.62   3199
## 30  51  5.05   2915
## 31  51  5.51   2656
## 32  51  6.02   2420
## 33  51  6.56   2205
## 34  51  7.16   2009
## 35  51  7.80   1831
## 36  51  8.49   1668
## 37  51  9.24   1520
## 38  51 10.05   1385
## 39  51 10.92   1262
## 40  51 11.86   1150
## 41  51 12.86   1048
## 42  51 13.94    954
## 43  51 15.09    870
## 44  51 16.31    792
## 45  51 17.61    722
## 46  51 18.99    658
## 47  51 20.46    599
## 48  51 22.00    546
## 49  51 23.62    498
## 50  51 25.31    454
## 51  51 27.08    413
## 52  51 28.93    376
## 53  51 30.83    343
## 54  51 32.80    313
## 55  51 34.83    285
## 56  51 36.89    260
## 57  51 39.00    236
## 58  51 41.13    215
## 59  51 43.28    196
## 60  51 45.43    179
## 61  51 47.58    163
## 62  51 49.71    148
## 63  51 51.81    135
## 64  51 53.87    123
## 65  51 55.88    112
## 66  51 57.83    102
## 67  51 59.70     93
## 68  51 61.50     85
## 69  51 63.21     77
## 70  51 64.84     71
## 71  51 66.37     64
## 72  51 67.80     59
## 73  51 69.13     53
## 74  51 70.37     49
## 75  51 71.51     44
## 76  51 72.56     40
## 77  51 73.52     37
## 78  51 74.39     34
## 79  51 75.18     31
## 80  51 75.89     28
## 81  51 76.53     25
## 82  51 77.10     23
## 83  51 77.61     21
## 84  51 78.06     19
## 85  51 78.46     17
## 86  51 78.81     16
## 87  51 79.13     15
## 88  51 79.40     13
## 89  51 79.64     12
## 90  51 79.86     11
## 91  51 80.04     10
## 92  51 80.21      9
## 93  51 80.35      8
## 94  51 80.48      8
## 95  51 80.59      7
## 96  51 80.68      6
## 97  51 80.77      6
## 98  51 80.84      5
## 99  51 80.91      5
## 100 51 80.97      4
```

---

In a single pipe:


```r
mod_lasso &lt;- linear_reg(mixture = 1, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet") %&gt;%
  fit(best3bench_kg ~ ., data = ipf_tr)

mod_lasso
```

```
## parsnip model object
## 
## Fit time:  40ms 
## 
## Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = "gaussian",      alpha = ~1) 
## 
##    Df  %Dev Lambda
## 1   0  0.00 43.280
## 2   1  8.53 39.440
## 3   2 16.06 35.940
## 4   2 24.11 32.740
## 5   2 30.79 29.830
## 6   2 36.33 27.180
## 7   2 40.94 24.770
## 8   2 44.76 22.570
## 9   2 47.93 20.560
## 10  2 50.57 18.740
## 11  2 52.76 17.070
## 12  2 54.57 15.560
## 13  2 56.08 14.170
## 14  2 57.33 12.910
## 15  3 58.88 11.770
## 16  4 60.97 10.720
## 17  6 62.99  9.769
## 18  6 64.92  8.902
## 19  6 66.52  8.111
## 20  7 67.92  7.390
## 21  7 69.20  6.734
## 22  8 70.56  6.136
## 23  8 72.10  5.590
## 24  8 73.38  5.094
## 25  8 74.44  4.641
## 26  8 75.32  4.229
## 27 10 76.07  3.853
## 28 11 76.77  3.511
## 29 11 77.37  3.199
## 30 13 77.89  2.915
## 31 13 78.34  2.656
## 32 14 78.71  2.420
## 33 14 79.05  2.205
## 34 16 79.35  2.009
## 35 17 79.61  1.831
## 36 17 79.82  1.668
## 37 19 80.02  1.520
## 38 19 80.19  1.385
## 39 19 80.34  1.262
## 40 19 80.46  1.150
## 41 19 80.56  1.048
## 42 19 80.64  0.955
## 43 23 80.72  0.870
## 44 23 80.79  0.792
## 45 25 80.86  0.722
## 46 25 80.91  0.658
## 47 27 80.96  0.599
## 48 28 81.01  0.546
## 49 29 81.04  0.498
## 50 29 81.08  0.454
## 51 29 81.10  0.413
## 52 31 81.12  0.376
## 53 32 81.14  0.343
## 54 33 81.16  0.313
## 55 35 81.18  0.285
## 56 39 81.19  0.260
## 57 40 81.20  0.236
## 58 40 81.22  0.215
## 59 42 81.23  0.196
## 60 43 81.25  0.179
## 61 43 81.30  0.163
## 62 43 81.34  0.148
## 63 44 81.38  0.135
## 64 45 81.41  0.123
## 65 45 81.44  0.112
## 66 44 81.47  0.102
## 67 46 81.50  0.093
## 68 47 81.51  0.085
## 69 47 81.54  0.077
## 70 49 81.55  0.071
## 71 48 81.57  0.064
## 72 48 81.58  0.059
## 73 48 81.59  0.053
## 74 48 81.59  0.049
## 75 48 81.60  0.044
## 76 49 81.60  0.040
## 77 50 81.61  0.037
## 78 48 81.62  0.034
## 79 50 81.63  0.031
## 80 50 81.63  0.028
## 81 50 81.64  0.025
## 82 50 81.64  0.023
## 83 50 81.65  0.021
## 84 50 81.65  0.019
```

---

Can also use `fit_xy()` a-la `sklearn`:


```r
mod_rf &lt;- rand_forest(mode = "regression", mtry = 4, trees = 50, min_n = 30) %&gt;%
  set_engine("ranger") %&gt;%
  fit_xy(x = ipf_tr[, -7],
         y = ipf_tr$best3bench_kg)

mod_rf
```

```
## parsnip model object
## 
## Fit time:  601ms 
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), num.trees = ~50, min.node.size = min_rows(~30, x), num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) 
## 
## Type:                             Regression 
## Number of trees:                  50 
## Sample size:                      19229 
## Number of independent variables:  12 
## Mtry:                             4 
## Target node size:                 30 
## Variable importance mode:         none 
## Splitrule:                        variance 
## OOB prediction error (MSE):       561.4695 
## R squared (OOB):                  0.8493916
```

---

Notice how easy it is to get the model's results in a tidy way using the `tidy()` function:


```r
tidy(mod_ridge)
```

```
## # A tibble: 52 x 3
##    term                  estimate penalty
##    &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)         -1958.       0.001
##  2 sexM                   58.9      0.001
##  3 eventSB               -30.6      0.001
##  4 eventSBD               -9.31     0.001
##  5 equipmentSingle-ply    29.3      0.001
##  6 age                    -0.0147   0.001
##  7 divisionJuniors        -4.13     0.001
##  8 divisionLight          -8.04     0.001
##  9 divisionMasters 1      -0.251    0.001
## 10 divisionMasters 2      -9.03     0.001
## # ... with 42 more rows
```

---

### Predicting


```r
results_test &lt;- mod_ridge %&gt;%
  predict(new_data = ipf_te, penalty = 0.001) %&gt;%
  mutate(
    truth = ipf_te$best3bench_kg,
    method = "Ridge"
  ) %&gt;%
  bind_rows(mod_lasso %&gt;%
    predict(new_data = ipf_te) %&gt;%
    mutate(
      truth = ipf_te$best3bench_kg,
      method = "Lasso"
  )) %&gt;%
  bind_rows(mod_rf %&gt;%
    predict(new_data = ipf_te) %&gt;%
    mutate(
      truth = ipf_te$best3bench_kg,
      method = "RF"
  ))

dim(results_test)
```

```
## [1] 38454     3
```

```r
head(results_test)
```

```
## # A tibble: 6 x 3
##   .pred truth method
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1  70.9  65   Ridge 
## 2  80.2 100   Ridge 
## 3  80.2  82.5 Ridge 
## 4  81.7  72.5 Ridge 
## 5  85.9 105   Ridge 
## 6  84.3  75   Ridge
```

---

### Comparing Models

The package `yardstick` has tons of performance [metrics](https://tidymodels.github.io/yardstick/articles/metric-types.html):


```r
results_test %&gt;%
  group_by(method) %&gt;%
  rmse(truth = truth, estimate = .pred)
```

```
## # A tibble: 3 x 4
##   method .metric .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 Lasso  rmse    standard        26.3
## 2 RF     rmse    standard        23.6
## 3 Ridge  rmse    standard        26.7
```

---


```r
results_test %&gt;%
  ggplot(aes(.pred, truth)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ method) +
  theme_bw()
```

&lt;img src="images/Tidymodels-Pred-vs-True-1.png" width="100%" /&gt;

---

### Tuning

Define your model spec, using `tune()` from the `tune` package for a parameter you wish to tune:


```r
mod_rf_spec &lt;- rand_forest(mode = "regression",
                           mtry = tune(),
                           min_n = tune(),
                           trees = 100) %&gt;%
  set_engine("ranger")
```

---

Define the `grid` on which you train your params, with the `dials` package:


```r
rf_grid &lt;- grid_regular(mtry(range(2, 10)), min_n(range(10, 30)),
                        levels = c(5, 3))

rf_grid
```

```
## # A tibble: 15 x 2
##     mtry min_n
##    &lt;int&gt; &lt;int&gt;
##  1     2    10
##  2     4    10
##  3     6    10
##  4     8    10
##  5    10    10
##  6     2    20
##  7     4    20
##  8     6    20
##  9     8    20
## 10    10    20
## 11     2    30
## 12     4    30
## 13     6    30
## 14     8    30
## 15    10    30
```

---

Split your data into a few folds for Cross Validation with `vfold_cv()` from the `rsample` package:


```r
cv_splits &lt;- vfold_cv(ipf_tr, v = 5)

cv_splits
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 2
##   splits               id   
##   &lt;list&gt;               &lt;chr&gt;
## 1 &lt;split [15.4K/3.8K]&gt; Fold1
## 2 &lt;split [15.4K/3.8K]&gt; Fold2
## 3 &lt;split [15.4K/3.8K]&gt; Fold3
## 4 &lt;split [15.4K/3.8K]&gt; Fold4
## 5 &lt;split [15.4K/3.8K]&gt; Fold5
```

---

Now perform cross validation with `tune_grid()` from the `tune` package:


```r
tune_res &lt;- tune_grid(mod_rf_spec,
                      recipe(best3bench_kg ~ ., data = ipf_tr),
                      resamples = cv_splits,
                      grid = rf_grid,
                      metrics = metric_set(rmse))
tune_res
```


```
## # Tuning results
## # 5-fold cross-validation 
## # A tibble: 5 x 4
##   splits               id    .metrics          .notes          
##   &lt;list&gt;               &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          
## 1 &lt;split [15.4K/3.8K]&gt; Fold1 &lt;tibble [15 x 6]&gt; &lt;tibble [0 x 1]&gt;
## 2 &lt;split [15.4K/3.8K]&gt; Fold2 &lt;tibble [15 x 6]&gt; &lt;tibble [0 x 1]&gt;
## 3 &lt;split [15.4K/3.8K]&gt; Fold3 &lt;tibble [15 x 6]&gt; &lt;tibble [0 x 1]&gt;
## 4 &lt;split [15.4K/3.8K]&gt; Fold4 &lt;tibble [15 x 6]&gt; &lt;tibble [0 x 1]&gt;
## 5 &lt;split [15.4K/3.8K]&gt; Fold5 &lt;tibble [15 x 6]&gt; &lt;tibble [0 x 1]&gt;
```

---


```r
tune_res$.metrics[[1]]
```

```
## # A tibble: 15 x 6
##     mtry min_n .metric .estimator .estimate .config              
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                
##  1     2    10 rmse    standard        24.5 Preprocessor1_Model01
##  2     4    10 rmse    standard        23.6 Preprocessor1_Model02
##  3     6    10 rmse    standard        23.7 Preprocessor1_Model03
##  4     8    10 rmse    standard        23.7 Preprocessor1_Model04
##  5    10    10 rmse    standard        23.8 Preprocessor1_Model05
##  6     2    20 rmse    standard        24.5 Preprocessor1_Model06
##  7     4    20 rmse    standard        23.4 Preprocessor1_Model07
##  8     6    20 rmse    standard        23.4 Preprocessor1_Model08
##  9     8    20 rmse    standard        23.4 Preprocessor1_Model09
## 10    10    20 rmse    standard        23.4 Preprocessor1_Model10
## 11     2    30 rmse    standard        24.6 Preprocessor1_Model11
## 12     4    30 rmse    standard        23.4 Preprocessor1_Model12
## 13     6    30 rmse    standard        23.3 Preprocessor1_Model13
## 14     8    30 rmse    standard        23.3 Preprocessor1_Model14
## 15    10    30 rmse    standard        23.3 Preprocessor1_Model15
```

---

Collect the mean metric across folds:


```r
estimates &lt;- collect_metrics(tune_res)

estimates
```

```
## # A tibble: 15 x 8
##     mtry min_n .metric .estimator  mean     n std_err .config              
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1     2    10 rmse    standard    24.8     5   0.163 Preprocessor1_Model01
##  2     4    10 rmse    standard    23.8     5   0.129 Preprocessor1_Model02
##  3     6    10 rmse    standard    23.8     5   0.129 Preprocessor1_Model03
##  4     8    10 rmse    standard    23.9     5   0.133 Preprocessor1_Model04
##  5    10    10 rmse    standard    23.9     5   0.135 Preprocessor1_Model05
##  6     2    20 rmse    standard    24.8     5   0.214 Preprocessor1_Model06
##  7     4    20 rmse    standard    23.6     5   0.141 Preprocessor1_Model07
##  8     6    20 rmse    standard    23.6     5   0.121 Preprocessor1_Model08
##  9     8    20 rmse    standard    23.7     5   0.143 Preprocessor1_Model09
## 10    10    20 rmse    standard    23.6     5   0.142 Preprocessor1_Model10
## 11     2    30 rmse    standard    24.9     5   0.162 Preprocessor1_Model11
## 12     4    30 rmse    standard    23.5     5   0.123 Preprocessor1_Model12
## 13     6    30 rmse    standard    23.5     5   0.132 Preprocessor1_Model13
## 14     8    30 rmse    standard    23.5     5   0.145 Preprocessor1_Model14
## 15    10    30 rmse    standard    23.6     5   0.132 Preprocessor1_Model15
```

---

Choose best paramter:


```r
estimates %&gt;%
  mutate(min_n = factor(min_n)) %&gt;%
  ggplot(aes(x = mtry, y = mean, color = min_n)) + 
  geom_point() + 
  geom_line() + 
  labs(y = "Mean RMSE") +
  theme_classic()
```

&lt;img src="images/Tidymodels-RMSE-Comp-1.png" width="100%" /&gt;

---

There are of course also methods for helping us choose best params and final model.


```r
best_rmse &lt;- tune_res %&gt;% select_best(metric = "rmse")
best_rmse
```

```
## # A tibble: 1 x 3
##    mtry min_n .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;                
## 1     6    30 Preprocessor1_Model13
```

See also `?select_by_one_std_err`.


```r
mod_rf_final &lt;- finalize_model(mod_rf_spec, best_rmse)
mod_rf_final
```

```
## Random Forest Model Specification (regression)
## 
## Main Arguments:
##   mtry = 6
##   trees = 100
##   min_n = 30
## 
## Computational engine: ranger
```

---


```r
mod_rf_final %&gt;%
  fit(best3bench_kg ~ ., data = ipf_tr) %&gt;%
  predict(new_data = ipf_te) %&gt;%
  mutate(truth = ipf_te$best3bench_kg)
```

```
## # A tibble: 12,818 x 2
##    .pred truth
##    &lt;dbl&gt; &lt;dbl&gt;
##  1  64.1  65  
##  2  73.8 100  
##  3  73.8  82.5
##  4  74.5  72.5
##  5  80.3 105  
##  6  80.2  75  
##  7  86.2 110  
##  8  87.9  85  
##  9  92.8 138. 
## 10  90.4 100  
## # ... with 12,808 more rows
```

---

class: section-slide

# `infer`: Tidy Statistics

---

### Statistical Q1

Is there a relation between men and women and the type of equipment they use in 2019? Assume observations are independent.


```r
sex_vs_equipment &lt;- ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  select(sex, equipment) %&gt;%
  table()

sex_vs_equipment
```

```
##    equipment
## sex Raw Single-ply
##   F 678        186
##   M 854        287
```


```r
prop.table(sex_vs_equipment, margin = 1)
```

```
##    equipment
## sex       Raw Single-ply
##   F 0.7847222  0.2152778
##   M 0.7484663  0.2515337
```

---

### Statistical Q2

Is there a difference between men and women age in 2019? Assume observations are independent.


```r
ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  group_by(sex) %&gt;% summarise(avg = mean(age), sd = sd(age), n = n())
```

```
## # A tibble: 2 x 4
##   sex     avg    sd     n
##   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 F      36.0  15.5   864
## 2 M      38.8  16.7  1141
```

---

### Same Problem!

Varied interface, varied output.


```r
prop.test(sex_vs_equipment[,1], rowSums(sex_vs_equipment))
```

```
## 
## 	2-sample test for equality of proportions with continuity correction
## 
## data:  sex_vs_equipment[, 1] out of rowSums(sex_vs_equipment)
## X-squared = 3.3872, df = 1, p-value = 0.0657
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  -0.001975717  0.074487646
## sample estimates:
##    prop 1    prop 2 
## 0.7847222 0.7484663
```

---


```r
t.test(age ~ sex, data = ipf_lifts %&gt;% filter(year == 2019))
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  age by sex
## t = -3.8797, df = 1921.8, p-value = 0.0001081
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -4.228319 -1.388844
## sample estimates:
## mean in group F mean in group M 
##        35.97801        38.78659
```

---

### The `generics::tidy()` Approach

(Also available when you load several other packages, like `broom` and `yardstick`)


```r
tidy(prop.test(sex_vs_equipment[,1], rowSums(sex_vs_equipment)))
```

```
## # A tibble: 1 x 9
##   estimate1 estimate2 statistic p.value parameter conf.low conf.high method
##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; 
## 1     0.785     0.748      3.39  0.0657         1 -0.00198    0.0745 2-sam~
## # ... with 1 more variable: alternative &lt;chr&gt;
```


```r
tidy(t.test(age ~ sex, data = ipf_lifts %&gt;% filter(year == 2019)))
```

```
## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1    -2.81      36.0      38.8     -3.88 1.08e-4     1922.    -4.23     -1.39
## # ... with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;
```

---

### The `infer` Approach

&gt; infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework

4 main verbs for a typical flow:

* `specify()` - dependent/independent variables, formula
* `hypothesize()` - declare the null hypothesis
* `generate()` - generate data reflecting the null hypothesis (the permutation/bootstrap approach)
* `calculate()` - calculate a distribution of statistics from the generated data, from which you can extract conclusion based on a p-value for example

---

### `infer` Diff in Proportions Test

Get the observed statistic (here manually in order to not confuse you, there *is* a way via `infer`):


```r
#    equipment
# sex Raw Single-ply
#   F 678        186
#   M 854        287
p_F &lt;- sex_vs_equipment[1, 1] / (sum(sex_vs_equipment[1, ]))
p_M &lt;- sex_vs_equipment[2, 1] / (sum(sex_vs_equipment[2, ]))
obs_diff &lt;- p_F - p_M
obs_diff
```

```
## [1] 0.03625596
```

---

Get distribution of the difference in proportions under null hypothesis


```r
diff_null_perm &lt;- ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  specify(equipment ~ sex, success = "Raw") %&gt;%
  hypothesize(null = "independence") %&gt;%
  generate(reps = 200, type = "permute") %&gt;%
  calculate(stat = "diff in props", order = c("F", "M"))

diff_null_perm
```

```
## # A tibble: 200 x 2
##    replicate      stat
##        &lt;int&gt;     &lt;dbl&gt;
##  1         1  0.0139  
##  2         2 -0.0207  
##  3         3  0.0200  
##  4         4 -0.000353
##  5         5 -0.0126  
##  6         6  0.0200  
##  7         7  0.0322  
##  8         8 -0.0614  
##  9         9 -0.00442 
## 10        10  0.00778 
## # ... with 190 more rows
```

---

Visualize the permuted difference null distribution and the p-value


```r
visualize(diff_null_perm) +
  shade_p_value(obs_stat = obs_diff, direction = "two_sided")
```

&lt;img src="images/Diff-in-Props-Null-1.png" width="50%" /&gt;

---

Get the actual p-value:


```r
diff_null_perm %&gt;% 
  get_p_value(obs_stat = obs_diff, direction = "two_sided")
```

```
## # A tibble: 1 x 1
##   p_value
##     &lt;dbl&gt;
## 1    0.07
```

---

### `infer` t Test (independent samples)

Get the observed statistic (here via `infer`):


```r
obs_t &lt;- ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  specify(age ~ sex) %&gt;%
  calculate(stat = "t", order = c("F", "M"))
obs_t
```

```
## # A tibble: 1 x 1
##    stat
##   &lt;dbl&gt;
## 1 -3.88
```


---

Get distribution of the t statistic under null hypothesis


```r
t_null_perm &lt;- ipf_lifts %&gt;%
  filter(year == 2019) %&gt;%
  specify(age ~ sex) %&gt;%
  hypothesize(null = "independence") %&gt;%
  generate(reps = 100, type = "permute") %&gt;%
  calculate(stat = "t", order = c("F", "M"))

t_null_perm
```

```
## # A tibble: 100 x 2
##    replicate   stat
##        &lt;int&gt;  &lt;dbl&gt;
##  1         1  0.316
##  2         2 -1.07 
##  3         3  0.424
##  4         4 -0.304
##  5         5 -1.02 
##  6         6 -0.259
##  7         7 -0.972
##  8         8  0.354
##  9         9 -1.51 
## 10        10 -1.42 
## # ... with 90 more rows
```

---

Visualize the permuted t statistic null distribution and the two-sided p-value


```r
visualize(t_null_perm) +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
```

&lt;img src="images/T-Null-1.png" width="50%" /&gt;

---

Get the actual p-value:


```r
t_null_perm %&gt;% 
  get_p_value(obs_stat = obs_t, direction = "two_sided")
```

&lt;pre style="color: red;"&gt;&lt;code&gt;## Warning: Please be cautious in reporting a p-value of 0. This result is an
## approximation based on the number of `reps` chosen in the `generate()` step. See
## `?get_p_value()` for more information.
&lt;/code&gt;&lt;/pre&gt;

```
## # A tibble: 1 x 1
##   p_value
##     &lt;dbl&gt;
## 1       0
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
