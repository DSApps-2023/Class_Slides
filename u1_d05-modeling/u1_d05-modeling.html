<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Modeling in the Tidyverse</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2022-12-25" />
    <script src="u1_d05-modeling_files/header-attrs-2.19/header-attrs.js"></script>
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Modeling in the Tidyverse

### Applications of Data Science - Class 5

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2022-12-25

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2023.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# The Problem

---

### Inconsistency, Inextensibility


```r
n &lt;- 10000
x1 &lt;- runif(n)
x2 &lt;- runif(n)
t &lt;- 1 + 2 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))
```


```r
glm(y ~ x1 + x2, family = "binomial")
```


```r
glmnet(as.matrix(cbind(x1, x2)), as.factor(y), family = "binomial")
```


```r
randomForest(as.factor(y) ~ x1 + x2)
```



```r
gbm(y ~ x1 + x2, data = data.frame(x1 = x1, x2 = x2, y = y))
```

😱

---

### Compare this with `sklearn`


```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier,
  GradientBoostingClassifier

LogisticRegression(penalty='none').fit(X, y)

LogisticRegression(penalty='l2', C=0.001).fit(X, y)

RandomForestClassifier(n_estimators=100).fit(X, y)

GradientBoostingClassifier(n_estimators=100).fit(X, y)
```

---

class: section-slide

# Detour: A Regression Problem

---

### Hungarian Blogs: Predicting Feedback

- Dataset was published as part of the [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/BlogFeedback) initiative
- Comes from [Buza 2014](http://www.cs.bme.hu/~buza/pdfs/gfkl2012_blogs.pdf)
- 280 numeric heavily engineered features on blogs and posts published in the last 72 hours
- Can we predict no. of comments in the next 24 hours?

&lt;img src="images/hungarian_blog.png" style="width: 70%" /&gt;

---

The raw data has over 50K rows: for each blog features like total comments until base time, weekday, words, etc.

We will be predicting `log(fb)` based on all features, no missing values:


```r
blogs_fb &lt;- read_csv("~/BlogFeedback/blogData_train.csv", col_names = FALSE)

blogs_fb &lt;- blogs_fb %&gt;%
  rename(fb = X281, blog_len = X62, sunday = X276) %&gt;%
  mutate(fb = log(fb + 1))

dim(blogs_fb)
```

```
## [1] 52397   281
```

---


```r
glimpse(blogs_fb)
```

```
## Rows: 52,397
## Columns: 281
## $ X1       &lt;dbl&gt; 40.30467, 40.30467, 40.30467, 40.30467, 40.30467, 40.30467, 4…
## $ X2       &lt;dbl&gt; 53.84566, 53.84566, 53.84566, 53.84566, 53.84566, 53.84566, 5…
## $ X3       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X4       &lt;dbl&gt; 401, 401, 401, 401, 401, 401, 401, 401, 401, 401, 401, 401, 4…
## $ X5       &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1…
## $ X6       &lt;dbl&gt; 15.52416, 15.52416, 15.52416, 15.52416, 15.52416, 15.52416, 1…
## $ X7       &lt;dbl&gt; 32.44188, 32.44188, 32.44188, 32.44188, 32.44188, 32.44188, 3…
## $ X8       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X9       &lt;dbl&gt; 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 3…
## $ X10      &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…
## $ X11      &lt;dbl&gt; 14.04423, 14.04423, 14.04423, 14.04423, 14.04423, 14.04423, 1…
## $ X12      &lt;dbl&gt; 32.61542, 32.61542, 32.61542, 32.61542, 32.61542, 32.61542, 3…
## $ X13      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X14      &lt;dbl&gt; 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 3…
## $ X15      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…
## $ X16      &lt;dbl&gt; 34.56757, 34.56757, 34.56757, 34.56757, 34.56757, 34.56757, 3…
## $ X17      &lt;dbl&gt; 48.47518, 48.47518, 48.47518, 48.47518, 48.47518, 48.47518, 4…
## $ X18      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X19      &lt;dbl&gt; 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 3…
## $ X20      &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1…
## $ X21      &lt;dbl&gt; 1.479934, 1.479934, 1.479934, 1.479934, 1.479934, 1.479934, 1…
## $ X22      &lt;dbl&gt; 46.18691, 46.18691, 46.18691, 46.18691, 46.18691, 46.18691, 4…
## $ X23      &lt;dbl&gt; -356, -356, -356, -356, -356, -356, -356, -356, -356, -356, -…
## $ X24      &lt;dbl&gt; 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 3…
## $ X25      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X26      &lt;dbl&gt; 1.076167, 1.076167, 1.076167, 1.076167, 1.076167, 1.076167, 1…
## $ X27      &lt;dbl&gt; 1.795416, 1.795416, 1.795416, 1.795416, 1.795416, 1.795416, 1…
## $ X28      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X29      &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1…
## $ X30      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X31      &lt;dbl&gt; 0.4004914, 0.4004914, 0.4004914, 0.4004914, 0.4004914, 0.4004…
## $ X32      &lt;dbl&gt; 1.078097, 1.078097, 1.078097, 1.078097, 1.078097, 1.078097, 1…
## $ X33      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X34      &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…
## $ X35      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X36      &lt;dbl&gt; 0.3775594, 0.3775594, 0.3775594, 0.3775594, 0.3775594, 0.3775…
## $ X37      &lt;dbl&gt; 1.07421, 1.07421, 1.07421, 1.07421, 1.07421, 1.07421, 1.07421…
## $ X38      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X39      &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…
## $ X40      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X41      &lt;dbl&gt; 0.972973, 0.972973, 0.972973, 0.972973, 0.972973, 0.972973, 0…
## $ X42      &lt;dbl&gt; 1.704671, 1.704671, 1.704671, 1.704671, 1.704671, 1.704671, 1…
## $ X43      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X44      &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…
## $ X45      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X46      &lt;dbl&gt; 0.02293202, 0.02293202, 0.02293202, 0.02293202, 0.02293202, 0…
## $ X47      &lt;dbl&gt; 1.521174, 1.521174, 1.521174, 1.521174, 1.521174, 1.521174, 1…
## $ X48      &lt;dbl&gt; -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -…
## $ X49      &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…
## $ X50      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X51      &lt;dbl&gt; 2, 6, 6, 2, 3, 6, 6, 3, 30, 30, 0, 0, 30, 0, 51, 10, 32, 10, …
## $ X52      &lt;dbl&gt; 2, 2, 2, 2, 1, 0, 0, 1, 27, 27, 0, 0, 30, 0, 51, 10, 2, 10, 5…
## $ X53      &lt;dbl&gt; 0, 4, 4, 0, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 30, 0, 0, 51,…
## $ X54      &lt;dbl&gt; 2, 5, 5, 2, 2, 5, 5, 2, 2, 2, 0, 0, 30, 0, 51, 10, 32, 10, 51…
## $ X55      &lt;dbl&gt; 2, -2, -2, 2, -1, -2, -2, -1, 26, 26, 0, 0, 30, 0, 51, 10, -2…
## $ X56      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 2, 3, 0, 2, 0, 3, 4, 0…
## $ X57      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 3, 0, 1, 0, 3, 1, 0…
## $ X58      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 3, 0…
## $ X59      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 2, 3, 0, 2, 0, 3, 4, 0…
## $ X60      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, -2, 1, 0, 3, 0, 0, 0, 3, -2,…
## $ X61      &lt;dbl&gt; 10, 35, 35, 10, 34, 59, 59, 34, 58, 58, 11, 35, 5, 59, 8, 14,…
## $ blog_len &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X63      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X64      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X65      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X66      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X67      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X68      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X69      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X70      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X71      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X72      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X73      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X74      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X75      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X76      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X77      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X78      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X79      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X80      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X81      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X82      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X83      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X84      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X85      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X86      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X87      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X88      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X89      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X90      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X91      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X92      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X93      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X94      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X95      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X96      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X97      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X98      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X99      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X100     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X101     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X102     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X103     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X104     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X105     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X106     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X107     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X108     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X109     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X110     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X111     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X112     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X113     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X114     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X115     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X116     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X117     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X118     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X119     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X120     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X121     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X122     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X123     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X124     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X125     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X126     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X127     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X128     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X129     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X130     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X131     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X132     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X133     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X134     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X135     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X136     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X137     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X138     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X139     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X140     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X141     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X142     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X143     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X144     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X145     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X146     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X147     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X148     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X149     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X150     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X151     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X152     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X153     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X154     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X155     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X156     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X157     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X158     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X159     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X160     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X161     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X162     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X163     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X164     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X165     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X166     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X167     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X168     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X169     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X170     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X171     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X172     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X173     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X174     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X175     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X176     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X177     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X178     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X179     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X180     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X181     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X182     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X183     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X184     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X185     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X186     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X187     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X188     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X189     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X190     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X191     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X192     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X193     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X194     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X195     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X196     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X197     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X198     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X199     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X200     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X201     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X202     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X203     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X204     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X205     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X206     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X207     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X208     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X209     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X210     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X211     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X212     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X213     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X214     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X215     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X216     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X217     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X218     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X219     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X220     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X221     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X222     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X223     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X224     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X225     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X226     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X227     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X228     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X229     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X230     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X231     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X232     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X233     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X234     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X235     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X236     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X237     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X238     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X239     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X240     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X241     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X242     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X243     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X244     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X245     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X246     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X247     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X248     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X249     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X250     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X251     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X252     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X253     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X254     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X255     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X256     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X257     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X258     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X259     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X260     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X261     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X262     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X263     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X264     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…
## $ X265     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0…
## $ X266     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1…
## $ X267     &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X268     &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X269     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X270     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0…
## $ X271     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1…
## $ X272     &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X273     &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X274     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X275     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0…
## $ sunday   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X277     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X278     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X279     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X280     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ fb       &lt;dbl&gt; 0.6931472, 0.0000000, 0.0000000, 0.6931472, 3.3322045, 0.0000…
```

---

See the dependent variable distribution:


```r
ggplot(blogs_fb, aes(fb)) +
  geom_histogram(fill = "red", alpha = 0.5, binwidth = 0.5) +
  theme_bw()
```

&lt;img src="images/Bench-Hist-1.png" width="100%" /&gt;

---

See it vs. say "length of post":


```r
ggplot(blogs_fb, aes(log(blog_len), fb)) +
  geom_point(color = "red", alpha = 0.5) +
  theme_bw()
```

&lt;img src="images/Bench-Age-Equipment-1.png" width="100%" /&gt;

---

class: section-slide

# End of Detour

---

# WARNING

.warning[
💡 What you're about to see is not a good modeling/prediction flow.

This is just an intro to tidy modeling.

Some of the issues with how things are done here will be raised, some will have to wait till later in the course.
]
---

class: section-slide

# The ~~Present~~ Past Solution: `caret`

---

### Split Data


```r
library(caret)

train_idx &lt;- createDataPartition(blogs_fb$fb,
                                 p = 0.6, list = FALSE)

blogs_tr &lt;- blogs_fb[train_idx, ]
blogs_te &lt;- blogs_fb[-train_idx, ]

library(glue)
glue("train no. of rows: {nrow(blogs_tr)}
     test no. of rows: {nrow(blogs_te)}")
```

```
## train no. of rows: 31439
## test no. of rows: 20958
```

Here you might consider some preprocessing.

`caret` has some nice documentation [here](https://topepo.github.io/caret/index.html).

---

### Tuning and Modeling

Define general methodology, e.g. 5-fold Cross-Validation:


```r
fit_control &lt;- trainControl(method = "cv", number = 5)

ridge_grid &lt;- expand.grid(alpha=0, lambda = 10^seq(-3, 1, length = 50))
lasso_grid &lt;- expand.grid(alpha=1, lambda = 10^seq(-3, 1, length = 50))
rf_grid &lt;- expand.grid(splitrule = "variance",
                       min.node.size = seq(10, 30, 10),
                       mtry = seq(10, 50, 20))

mod_ridge &lt;- train(fb ~ ., data = blogs_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = ridge_grid,
                metric = "RMSE")

mod_lasso &lt;- train(fb ~ ., data = blogs_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = lasso_grid,
                metric = "RMSE")

mod_rf &lt;- train(fb ~ ., data = blogs_tr, method = "ranger",
                trControl = fit_control, tuneGrid = rf_grid,
                num.trees = 50, metric = "RMSE")
```

---

### Evaluating Models


```r
mod_ridge
```

```
## glmnet 
## 
## 31439 samples
##   280 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 25152, 25152, 25151, 25151, 25150 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE       Rsquared   MAE      
##    0.001000000  0.8437879  0.4470115  0.5892297
##    0.001206793  0.8437879  0.4470115  0.5892297
##    0.001456348  0.8437879  0.4470115  0.5892297
##    0.001757511  0.8437879  0.4470115  0.5892297
##    0.002120951  0.8437879  0.4470115  0.5892297
##    0.002559548  0.8437879  0.4470115  0.5892297
##    0.003088844  0.8437879  0.4470115  0.5892297
##    0.003727594  0.8437879  0.4470115  0.5892297
##    0.004498433  0.8437879  0.4470115  0.5892297
##    0.005428675  0.8437879  0.4470115  0.5892297
##    0.006551286  0.8437879  0.4470115  0.5892297
##    0.007906043  0.8437879  0.4470115  0.5892297
##    0.009540955  0.8437879  0.4470115  0.5892297
##    0.011513954  0.8437879  0.4470115  0.5892297
##    0.013894955  0.8437879  0.4470115  0.5892297
##    0.016768329  0.8437879  0.4470115  0.5892297
##    0.020235896  0.8437879  0.4470115  0.5892297
##    0.024420531  0.8437879  0.4470115  0.5892297
##    0.029470517  0.8437879  0.4470115  0.5892297
##    0.035564803  0.8437879  0.4470115  0.5892297
##    0.042919343  0.8437879  0.4470115  0.5892297
##    0.051794747  0.8437879  0.4470115  0.5892297
##    0.062505519  0.8438871  0.4468879  0.5893096
##    0.075431201  0.8453020  0.4451016  0.5904108
##    0.091029818  0.8467748  0.4432529  0.5915339
##    0.109854114  0.8483028  0.4413509  0.5926947
##    0.132571137  0.8498856  0.4394027  0.5938961
##    0.159985872  0.8515301  0.4374083  0.5951278
##    0.193069773  0.8532471  0.4353644  0.5964250
##    0.232995181  0.8550479  0.4332696  0.5978120
##    0.281176870  0.8569811  0.4310722  0.5993268
##    0.339322177  0.8590936  0.4287245  0.6010566
##    0.409491506  0.8614298  0.4261814  0.6030858
##    0.494171336  0.8640477  0.4233808  0.6054926
##    0.596362332  0.8670170  0.4202310  0.6083554
##    0.719685673  0.8703936  0.4166531  0.6117868
##    0.868511374  0.8742256  0.4125856  0.6158486
##    1.048113134  0.8785464  0.4079641  0.6205362
##    1.264855217  0.8833736  0.4027425  0.6258051
##    1.526417967  0.8886948  0.3969209  0.6316116
##    1.842069969  0.8945017  0.3904903  0.6378787
##    2.222996483  0.9007584  0.3834886  0.6444836
##    2.682695795  0.9074097  0.3759880  0.6513238
##    3.237457543  0.9143996  0.3680815  0.6583496
##    3.906939937  0.9216715  0.3598830  0.6655178
##    4.714866363  0.9291747  0.3515212  0.6728143
##    5.689866029  0.9368696  0.3431343  0.6801875
##    6.866488450  0.9447313  0.3348606  0.6876204
##    8.286427729  0.9527490  0.3268247  0.6951284
##   10.000000000  0.9609256  0.3191272  0.7026954
## 
## Tuning parameter 'alpha' was held constant at a value of 0
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 0.05179475.
```

---


```r
mod_lasso
```

```
## glmnet 
## 
## 31439 samples
##   280 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 25151, 25152, 25151, 25151, 25151 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE       Rsquared   MAE      
##    0.001000000  0.8309697  0.4639544  0.5776893
##    0.001206793  0.8313625  0.4634437  0.5780418
##    0.001456348  0.8319219  0.4627230  0.5785559
##    0.001757511  0.8326338  0.4618113  0.5791961
##    0.002120951  0.8335103  0.4606975  0.5799814
##    0.002559548  0.8346239  0.4592823  0.5809361
##    0.003088844  0.8362502  0.4572036  0.5822341
##    0.003727594  0.8385627  0.4542227  0.5838802
##    0.004498433  0.8404367  0.4518214  0.5852548
##    0.005428675  0.8425197  0.4491466  0.5866328
##    0.006551286  0.8451632  0.4457254  0.5884761
##    0.007906043  0.8477014  0.4424336  0.5903817
##    0.009540955  0.8504094  0.4389288  0.5924214
##    0.011513954  0.8531640  0.4353596  0.5945062
##    0.013894955  0.8558311  0.4319254  0.5965421
##    0.016768329  0.8582994  0.4287648  0.5985634
##    0.020235896  0.8596274  0.4272297  0.5998137
##    0.024420531  0.8609368  0.4258249  0.6011417
##    0.029470517  0.8623160  0.4244684  0.6026516
##    0.035564803  0.8639456  0.4229587  0.6045897
##    0.042919343  0.8661700  0.4208570  0.6072649
##    0.051794747  0.8688452  0.4184697  0.6107041
##    0.062505519  0.8717076  0.4163626  0.6149721
##    0.075431201  0.8750663  0.4143487  0.6203892
##    0.091029818  0.8795191  0.4118316  0.6271830
##    0.109854114  0.8844985  0.4102871  0.6351105
##    0.132571137  0.8913966  0.4082623  0.6448296
##    0.159985872  0.9013526  0.4047088  0.6570200
##    0.193069773  0.9156640  0.3981551  0.6723674
##    0.232995181  0.9361215  0.3852654  0.6917141
##    0.281176870  0.9637406  0.3617917  0.7150701
##    0.339322177  0.9906564  0.3529242  0.7355546
##    0.409491506  1.0262798  0.3399598  0.7602455
##    0.494171336  1.0726643  0.3014576  0.7901998
##    0.596362332  1.1241489  0.2951708  0.8187624
##    0.719685673  1.1347255        NaN  0.8244471
##    0.868511374  1.1347255        NaN  0.8244471
##    1.048113134  1.1347255        NaN  0.8244471
##    1.264855217  1.1347255        NaN  0.8244471
##    1.526417967  1.1347255        NaN  0.8244471
##    1.842069969  1.1347255        NaN  0.8244471
##    2.222996483  1.1347255        NaN  0.8244471
##    2.682695795  1.1347255        NaN  0.8244471
##    3.237457543  1.1347255        NaN  0.8244471
##    3.906939937  1.1347255        NaN  0.8244471
##    4.714866363  1.1347255        NaN  0.8244471
##    5.689866029  1.1347255        NaN  0.8244471
##    6.866488450  1.1347255        NaN  0.8244471
##    8.286427729  1.1347255        NaN  0.8244471
##   10.000000000  1.1347255        NaN  0.8244471
## 
## Tuning parameter 'alpha' was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.001.
```

---


```r
mod_rf
```

```
## Random Forest 
## 
## 31439 samples
##   280 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 25152, 25151, 25150, 25151, 25152 
## Resampling results across tuning parameters:
## 
##   min.node.size  mtry  RMSE       Rsquared   MAE      
##   10             10    0.7006928  0.6257170  0.4491360
##   10             30    0.6601092  0.6627912  0.4112107
##   10             50    0.6510154  0.6713434  0.4027723
##   20             10    0.7041001  0.6229894  0.4532592
##   20             30    0.6616848  0.6617373  0.4147985
##   20             50    0.6522558  0.6704663  0.4064939
##   30             10    0.7125997  0.6142418  0.4593610
##   30             30    0.6640936  0.6596200  0.4180021
##   30             50    0.6557245  0.6672094  0.4092338
## 
## Tuning parameter 'splitrule' was held constant at a value of variance
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 50, splitrule = variance
##  and min.node.size = 10.
```

---


```r
plot(mod_ridge)
```

&lt;img src="images/Ridge-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_lasso)
```

&lt;img src="images/Lasso-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_rf)
```

&lt;img src="images/RF-CV-1.png" width="80%" /&gt;

---

### Comparing Models


```r
resamps &lt;- resamples(list(Ridge = mod_ridge, Lasso = mod_lasso,
                          RF = mod_rf))
summary(resamps)
```

```
## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: Ridge, Lasso, RF 
## Number of resamples: 5 
## 
## MAE 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.5770477 0.5812776 0.5857398 0.5892297 0.5992260 0.6028572    0
## Lasso 0.5643570 0.5744683 0.5807233 0.5776893 0.5808192 0.5880787    0
## RF    0.3957418 0.3969833 0.4018680 0.4027723 0.4038781 0.4153904    0
## 
## RMSE 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.8199203 0.8284166 0.8443990 0.8437879 0.8555938 0.8706100    0
## Lasso 0.8097263 0.8266005 0.8304597 0.8309697 0.8339804 0.8540817    0
## RF    0.6428488 0.6450360 0.6493624 0.6510154 0.6519068 0.6659228    0
## 
## Rsquared 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.4166000 0.4432796 0.4553274 0.4470115 0.4587103 0.4611401    0
## Lasso 0.4546575 0.4571328 0.4661696 0.4639544 0.4688337 0.4729786    0
## RF    0.6632821 0.6691068 0.6713451 0.6713434 0.6764046 0.6765781    0
```

---


```r
dotplot(resamps, metric = "RMSE")
```

&lt;img src="images/Caret-RMSE-Comp-1.png" width="100%" /&gt;

---

### Predicting


```r
pred_ridge &lt;- predict(mod_ridge, newdata = blogs_te)
pred_lasso &lt;- predict(mod_lasso, newdata = blogs_te)
pred_rf &lt;- predict(mod_rf, newdata = blogs_te)

rmse_ridge &lt;- RMSE(pred_ridge, blogs_te$fb)
rmse_lasso &lt;- RMSE(pred_lasso, blogs_te$fb)
rmse_rf &lt;- RMSE(pred_rf, blogs_te$fb)

glue("Test RMSE Ridge: {format(rmse_ridge, digits = 3)}
     Test RMSE Lassoe: {format(rmse_lasso, digits = 3)}
     Test RMSE RF: {format(rmse_rf, digits = 3)}")
```

```
## Test RMSE Ridge: 0.85
## Test RMSE Lassoe: 0.839
## Test RMSE RF: 0.645
```

---


```r
bind_rows(
  tibble(method = "Ridge", pred = pred_ridge, true = blogs_te$fb),
  tibble(method = "Lasso", pred = pred_lasso, true = blogs_te$fb),
  tibble(method = "RF", pred = pred_rf, true = blogs_te$fb)) %&gt;%
  ggplot(aes(true, pred)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ factor(method, levels = c("Ridge", "Lasso", "RF"))) +
  theme_bw()
```

&lt;img src="images/Caret-Pred-vs-True-1.png" width="100%" /&gt;

---

class: section-slide

# The ~~Future~~ Present Solution: `tidymodels`

#### Inspired by [Julia Silge](https://juliasilge.com/blog/intro-tidymodels/)

---

### Packages under tidymodels

- `parsnip`: tidy `caret`
- `dials` and `tune`: specifying and tuning model parameters
- `rsample`: sampling, data partitioning
- `recipes`, `embed`, `themis`: preprocessing and creating model matrices
- `infer`: tidy statistics
- `yardstick`: measuring models performance
- `broom`: convert models output into tidy tibbles

And [more](https://www.tidymodels.org/).

.warning[
⚠️ All `tidymodels` packages are under development!
]

---

### Split Data

The `initial_split()` function is from the `rsample` package:


```r
library(tidymodels)

blogs_split_obj &lt;- blogs_fb %&gt;%
  initial_split(prop = 0.6)

print(blogs_split_obj)
```

```
## &lt;Training/Testing/Total&gt;
## &lt;31438/20959/52397&gt;
```

```r
blogs_tr &lt;- training(blogs_split_obj)
blogs_te &lt;- testing(blogs_split_obj)

glue("train no. of rows: {nrow(blogs_tr)}
     test no. of rows: {nrow(blogs_te)}")
```

```
## train no. of rows: 31438
## test no. of rows: 20959
```

---

### Preprocess .font80percent[(but we're not gonna use it)]

The `recipe()` function is from the `recipes` package. It allows you to specify a sklearn-like pipe you can later apply to any dataset, including all preprocessing steps:


```r
blogs_rec &lt;- recipe(fb ~ ., data = blogs_tr)
blogs_rec
```

```
## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor        280
```

`recipes` contains more preprocessing [`step_`s](https://tidymodels.github.io/recipes/reference/index.html) than you imagine:


```r
blogs_rec &lt;-  blogs_rec %&gt;%
  step_zv(all_numeric()) %&gt;%
  step_normalize(all_numeric())
```

---

After you have your `recipe` you need to `prep()` materials...


```r
blogs_rec &lt;- blogs_rec %&gt;% prep(blogs_tr)

blogs_rec
```

```
## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor        280
## 
## Training data contained 31438 data points and no missing data.
## 
## Operations:
## 
## Zero variance filter removed X13, X33, X38, X278 [trained]
## Centering and scaling for X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X... [trained]
```

At this point our `recipe` has all necessary `sd` and `mean`s for numeric variables.

---


```r
blogs_rec$var_info
```

```
## # A tibble: 281 × 4
##    variable type      role      source  
##    &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   
##  1 X1       &lt;chr [2]&gt; predictor original
##  2 X2       &lt;chr [2]&gt; predictor original
##  3 X3       &lt;chr [2]&gt; predictor original
##  4 X4       &lt;chr [2]&gt; predictor original
##  5 X5       &lt;chr [2]&gt; predictor original
##  6 X6       &lt;chr [2]&gt; predictor original
##  7 X7       &lt;chr [2]&gt; predictor original
##  8 X8       &lt;chr [2]&gt; predictor original
##  9 X9       &lt;chr [2]&gt; predictor original
## 10 X10      &lt;chr [2]&gt; predictor original
## # … with 271 more rows
```

---


```r
blogs_rec$steps[[2]]$means |&gt; head()
```

```
##         X1         X2         X3         X4         X5         X6 
##  39.292699  46.747639   0.373497 339.760767  24.542496  15.141135
```

```r
blogs_rec$steps[[2]]$sds |&gt; head()
```

```
##         X1         X2         X3         X4         X5         X6 
##  78.334982  61.711279   8.025361 438.824228  69.138357  31.887552
```

---

And then we `bake()` (or [`juice()`](https://tidymodels.github.io/recipes/reference/juice.html)):


```r
blogs_tr2 &lt;- blogs_rec %&gt;% bake(blogs_tr)
blogs_te2 &lt;- blogs_rec %&gt;% bake(blogs_te)

glue("mean of comments in orig training: {format(mean(blogs_tr$X51), digits = 3)}, sd: {format(sd(blogs_tr$X51), digits = 3)}
     mean of comments in baked training: {format(mean(blogs_tr2$X51), digits = 1)}, sd: {format(sd(blogs_tr2$X51), digits = 3)}")
```

```
## mean of comments in orig training: 39.1, sd: 111
## mean of comments in baked training: 9e-18, sd: 1
```

```r
glue("mean of comments in orig testing: {format(mean(blogs_te$X51), digits = 3)}, sd: {format(sd(blogs_te$X51), digits = 3)}
     mean of comments in baked testing: {format(mean(blogs_te2$X51), digits = 1)}, sd: {format(sd(blogs_te2$X51), digits = 3)}")
```

```
## mean of comments in orig testing: 39.9, sd: 112
## mean of comments in baked testing: 0.008, sd: 1.01
```

---

Or you can do it all in a single pipe:


```r
blogs_rec &lt;- recipe(fb ~ ., data = blogs_tr) %&gt;%
  step_zv(all_numeric()) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  prep(blogs_tr)

blogs_tr2 &lt;- blogs_rec %&gt;% bake(blogs_tr)
blogs_te2 &lt;- blogs_rec %&gt;% bake(blogs_te)

glue("mean of comments in orig training: {format(mean(blogs_tr$X51), digits = 3)}, sd: {format(sd(blogs_tr$X51), digits = 3)}
     mean of comments in baked training: {format(mean(blogs_tr2$X51), digits = 1)}, sd: {format(sd(blogs_tr2$X51), digits = 3)}")
```

```
## mean of comments in orig training: 39.1, sd: 111
## mean of comments in baked training: 9e-18, sd: 1
```

```r
glue("mean of comments in orig testing: {format(mean(blogs_te$X51), digits = 3)}, sd: {format(sd(blogs_te$X51), digits = 3)}
     mean of comments in baked testing: {format(mean(blogs_te2$X51), digits = 1)}, sd: {format(sd(blogs_te2$X51), digits = 3)}")
```

```
## mean of comments in orig testing: 39.9, sd: 112
## mean of comments in baked testing: 0.008, sd: 1.01
```

---

#### Fast Forward 10 weeks from now...


```r
rec_int_topints &lt;- recipe(pets ~ ., data = okcupid_tr) %&gt;%
  step_textfeature(essays, prefix = "t",
                   extract_functions = my_text_funs) %&gt;%
  update_role(essays, new_role = "discarded") %&gt;%
  step_mutate_at(starts_with("t_"), fn = ~ifelse(is.na(.x), 0, .x)) %&gt;%
  step_log(income, starts_with("len_"), starts_with("t_"),
           -t_essays_sent_bing, offset = 1) %&gt;%
  step_meanimpute(income) %&gt;%
  step_other(
    all_nominal(), -has_role("discarded"), -all_outcomes(),
    other = "all_else", threshold = 0.1) %&gt;%
  step_novel(
    all_nominal(), -has_role("discarded"), -all_outcomes()) %&gt;%
  step_modeimpute(all_nominal(), -has_role("discarded"), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(),
             -has_role("discarded"), one_hot = FALSE) %&gt;%
  step_interact(topint_ints) %&gt;%
  step_nzv(all_numeric(), freq_cut = 99/1) %&gt;%
  step_upsample(pets, over_ratio = 1, seed = 42)
```

---

### Modeling

For now let us use the original `blogs_tr` data.

Functions `linear_reg()` and `set_engine()` are from the `parsnip` package:


```r
mod_ridge_spec &lt;- linear_reg(mixture = 0, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet")

mod_ridge_spec
```

```
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0.001
##   mixture = 0
## 
## Computational engine: glmnet
```

---


```r
mod_ridge &lt;- mod_ridge_spec %&gt;%
  fit(fb ~ ., data = blogs_tr)

mod_ridge
```

```
## parsnip model object
## 
## 
## Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = "gaussian",      alpha = ~0) 
## 
##      Df  %Dev Lambda
## 1   276  0.00 615.10
## 2   276  2.59 560.50
## 3   276  2.82 510.70
## 4   276  3.07 465.30
## 5   276  3.34 424.00
## 6   276  3.63 386.30
## 7   276  3.95 352.00
## 8   276  4.28 320.70
## 9   276  4.64 292.20
## 10  276  5.03 266.30
## 11  276  5.44 242.60
## 12  276  5.88 221.10
## 13  276  6.34 201.40
## 14  276  6.84 183.50
## 15  276  7.36 167.20
## 16  276  7.91 152.40
## 17  276  8.49 138.80
## 18  276  9.09 126.50
## 19  276  9.72 115.30
## 20  276 10.38 105.00
## 21  276 11.06  95.70
## 22  276 11.76  87.19
## 23  276 12.48  79.45
## 24  276 13.22  72.39
## 25  276 13.97  65.96
## 26  276 14.74  60.10
## 27  276 15.51  54.76
## 28  276 16.29  49.90
## 29  276 17.07  45.46
## 30  276 17.86  41.42
## 31  276 18.64  37.74
## 32  276 19.42  34.39
## 33  276 20.19  31.34
## 34  276 20.95  28.55
## 35  276 21.70  26.02
## 36  276 22.45  23.70
## 37  276 23.17  21.60
## 38  276 23.89  19.68
## 39  276 24.59  17.93
## 40  276 25.28  16.34
## 41  276 25.95  14.89
## 42  276 26.62  13.56
## 43  276 27.27  12.36
## 44  276 27.91  11.26
## 45  276 28.54  10.26
## 46  276 29.16   9.35
## 47  276 29.78   8.52
## 48  276 30.38   7.76
## 49  276 30.98   7.07
## 50  276 31.56   6.44
## 51  276 32.14   5.87
## 52  276 32.71   5.35
## 53  276 33.27   4.88
## 54  276 33.83   4.44
## 55  276 34.37   4.05
## 56  276 34.91   3.69
## 57  276 35.43   3.36
## 58  276 35.94   3.06
## 59  276 36.44   2.79
## 60  276 36.93   2.54
## 61  276 37.40   2.32
## 62  276 37.85   2.11
## 63  276 38.30   1.92
## 64  276 38.72   1.75
## 65  276 39.13   1.60
## 66  276 39.52   1.45
## 67  276 39.89   1.32
## 68  276 40.25   1.21
## 69  276 40.59   1.10
## 70  276 40.91   1.00
## 71  276 41.21   0.91
## 72  276 41.49   0.83
## 73  276 41.76   0.76
## 74  276 42.01   0.69
## 75  276 42.25   0.63
## 76  276 42.47   0.57
## 77  276 42.68   0.52
## 78  276 42.87   0.48
## 79  276 43.06   0.43
## 80  276 43.23   0.40
## 81  276 43.39   0.36
## 82  276 43.55   0.33
## 83  276 43.69   0.30
## 84  276 43.83   0.27
## 85  276 43.97   0.25
## 86  276 44.10   0.23
## 87  276 44.22   0.21
## 88  276 44.34   0.19
## 89  276 44.46   0.17
## 90  276 44.58   0.16
## 91  276 44.69   0.14
## 92  276 44.80   0.13
## 93  276 44.91   0.12
## 94  276 45.02   0.11
## 95  276 45.12   0.10
## 96  276 45.22   0.09
## 97  276 45.33   0.08
## 98  276 45.42   0.07
## 99  276 45.52   0.07
## 100 276 45.62   0.06
```

---

In a single pipe:


```r
mod_lasso &lt;- linear_reg(mixture = 1, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet") %&gt;%
  fit(fb ~ ., data = blogs_tr)

mod_lasso
```

```
## parsnip model object
## 
## 
## Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = "gaussian",      alpha = ~1) 
## 
##      Df  %Dev  Lambda
## 1     0  0.00 0.61510
## 2     1  4.96 0.56050
## 3     1  9.07 0.51070
## 4     2 13.01 0.46530
## 5     3 16.88 0.42400
## 6     3 20.11 0.38630
## 7     3 22.78 0.35200
## 8     3 25.00 0.32070
## 9     3 26.84 0.29220
## 10    4 29.14 0.26630
## 11    4 31.20 0.24260
## 12    4 32.91 0.22110
## 13    4 34.33 0.20140
## 14    5 35.51 0.18350
## 15    5 36.50 0.16720
## 16    5 37.31 0.15240
## 17    5 37.99 0.13880
## 18    5 38.55 0.12650
## 19    5 39.02 0.11530
## 20    5 39.41 0.10500
## 21    5 39.73 0.09570
## 22    6 40.01 0.08719
## 23    6 40.32 0.07945
## 24    7 40.59 0.07239
## 25    8 40.85 0.06596
## 26    8 41.07 0.06010
## 27    9 41.25 0.05476
## 28   12 41.45 0.04990
## 29   12 41.64 0.04546
## 30   14 41.80 0.04142
## 31   15 41.95 0.03774
## 32   17 42.09 0.03439
## 33   19 42.22 0.03134
## 34   20 42.34 0.02855
## 35   22 42.43 0.02602
## 36   24 42.53 0.02370
## 37   30 42.63 0.02160
## 38   34 42.73 0.01968
## 39   37 42.83 0.01793
## 40   41 43.01 0.01634
## 41   43 43.18 0.01489
## 42   47 43.33 0.01356
## 43   55 43.49 0.01236
## 44   60 43.66 0.01126
## 45   71 43.82 0.01026
## 46   91 44.03 0.00935
## 47  103 44.28 0.00852
## 48  104 44.53 0.00776
## 49  113 44.71 0.00707
## 50  124 44.88 0.00644
## 51  133 45.03 0.00587
## 52  141 45.24 0.00535
## 53  152 45.42 0.00488
## 54  163 45.60 0.00444
## 55  166 45.81 0.00405
## 56  170 46.03 0.00369
## 57  179 46.23 0.00336
## 58  181 46.41 0.00306
## 59  186 46.55 0.00279
## 60  189 46.67 0.00254
## 61  192 46.78 0.00232
## 62  197 46.87 0.00211
## 63  195 46.95 0.00192
## 64  207 47.02 0.00175
## 65  208 47.09 0.00160
## 66  211 47.15 0.00145
## 67  218 47.20 0.00132
## 68  222 47.25 0.00121
## 69  228 47.30 0.00110
## 70  226 47.33 0.00100
## 71  228 47.37 0.00091
## 72  228 47.40 0.00083
## 73  233 47.42 0.00076
## 74  237 47.44 0.00069
## 75  242 47.48 0.00063
## 76  242 47.54 0.00057
## 77  242 47.58 0.00052
## 78  240 47.62 0.00048
## 79  244 47.65 0.00043
## 80  244 47.69 0.00040
## 81  245 47.72 0.00036
## 82  244 47.75 0.00033
## 83  245 47.77 0.00030
## 84  252 47.80 0.00027
## 85  251 47.81 0.00025
## 86  253 47.83 0.00023
## 87  255 47.85 0.00021
## 88  259 47.85 0.00019
## 89  258 47.87 0.00017
## 90  258 47.88 0.00016
## 91  256 47.88 0.00014
## 92  258 47.90 0.00013
## 93  258 47.91 0.00012
## 94  258 47.92 0.00011
## 95  260 47.93 0.00010
## 96  260 47.93 0.00009
## 97  261 47.95 0.00008
## 98  260 47.96 0.00007
## 99  260 47.97 0.00007
## 100 260 47.97 0.00006
```

---

Can also use `fit_xy()` a-la `sklearn`:


```r
mod_rf &lt;- rand_forest(mode = "regression", mtry = 50, trees = 50, min_n = 10) %&gt;%
  set_engine("ranger") %&gt;%
  fit_xy(x = blogs_tr[, -281],
         y = blogs_tr$fb)

mod_rf
```

```
## parsnip model object
## 
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~50,      x), num.trees = ~50, min.node.size = min_rows(~10, x), num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) 
## 
## Type:                             Regression 
## Number of trees:                  50 
## Sample size:                      31438 
## Number of independent variables:  280 
## Mtry:                             50 
## Target node size:                 10 
## Variable importance mode:         none 
## Splitrule:                        variance 
## OOB prediction error (MSE):       0.4218094 
## R squared (OOB):                  0.674542
```

---

Notice how easy it is to get the model's results in a tidy way using the `tidy()` function:


```r
tidy(mod_ridge)
```

```
## # A tibble: 281 × 3
##    term         estimate penalty
##    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)  0.675      0.001
##  2 X1           0.00134    0.001
##  3 X2           0.00120    0.001
##  4 X3           0.000173   0.001
##  5 X4           0.000337   0.001
##  6 X5           0.000168   0.001
##  7 X6           0.00227    0.001
##  8 X7          -0.00149    0.001
##  9 X8           0.0675     0.001
## 10 X9          -0.000134   0.001
## # … with 271 more rows
```

---

### Predicting


```r
results_test &lt;- mod_ridge %&gt;%
  predict(new_data = blogs_te, penalty = 0.001) %&gt;%
  mutate(
    truth = blogs_te$fb,
    method = "Ridge"
  ) %&gt;%
  bind_rows(mod_lasso %&gt;%
    predict(new_data = blogs_te) %&gt;%
    mutate(
      truth = blogs_te$fb,
      method = "Lasso"
  )) %&gt;%
  bind_rows(mod_rf %&gt;%
    predict(new_data = blogs_te) %&gt;%
    mutate(
      truth = blogs_te$fb,
      method = "RF"
  ))

dim(results_test)
```

```
## [1] 62877     3
```

```r
head(results_test)
```

```
## # A tibble: 6 × 3
##   .pred truth method
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1 0.884 0.693 Ridge 
## 2 0.566 0     Ridge 
## 3 0.534 3.33  Ridge 
## 4 0.210 0     Ridge 
## 5 0.525 2.30  Ridge 
## 6 0.525 2.30  Ridge
```

---

### Comparing Models

The package `yardstick` has tons of performance [metrics](https://tidymodels.github.io/yardstick/articles/metric-types.html):


```r
results_test %&gt;%
  group_by(method) %&gt;%
  rmse(truth = truth, estimate = .pred)
```

```
## # A tibble: 3 × 4
##   method .metric .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 Lasso  rmse    standard       0.835
## 2 RF     rmse    standard       0.646
## 3 Ridge  rmse    standard       0.847
```

---


```r
results_test %&gt;%
  ggplot(aes(truth, .pred)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ factor(method, levels = c("Ridge", "Lasso", "RF"))) +
  theme_bw()
```

&lt;img src="images/Tidymodels-Pred-vs-True-1.png" width="100%" /&gt;

---

### Tuning

Define your model spec, using `tune()` from the `tune` package for a parameter you wish to tune:


```r
mod_rf_spec &lt;- rand_forest(mode = "regression",
                           mtry = tune(),
                           min_n = tune(),
                           trees = 100) %&gt;%
  set_engine("ranger")
```

---

Define the `grid` on which you train your params, with the `dials` package:


```r
rf_grid &lt;- grid_regular(mtry(range(10, 70)), min_n(range(10, 30)),
                        levels = c(4, 3))

rf_grid
```

```
## # A tibble: 12 × 2
##     mtry min_n
##    &lt;int&gt; &lt;int&gt;
##  1    10    10
##  2    30    10
##  3    50    10
##  4    70    10
##  5    10    20
##  6    30    20
##  7    50    20
##  8    70    20
##  9    10    30
## 10    30    30
## 11    50    30
## 12    70    30
```

---

Split your data into a few folds for Cross Validation with `vfold_cv()` from the `rsample` package:


```r
cv_splits &lt;- vfold_cv(blogs_tr, v = 5)

cv_splits
```

```
## #  5-fold cross-validation 
## # A tibble: 5 × 2
##   splits               id   
##   &lt;list&gt;               &lt;chr&gt;
## 1 &lt;split [25150/6288]&gt; Fold1
## 2 &lt;split [25150/6288]&gt; Fold2
## 3 &lt;split [25150/6288]&gt; Fold3
## 4 &lt;split [25151/6287]&gt; Fold4
## 5 &lt;split [25151/6287]&gt; Fold5
```

---

Now perform cross validation with `tune_grid()` from the `tune` package:


```r
tune_res &lt;- tune_grid(mod_rf_spec,
                      recipe(fb ~ ., data = blogs_tr),
                      resamples = cv_splits,
                      grid = rf_grid,
                      metrics = metric_set(rmse))
tune_res
```


```
## # Tuning results
## # 5-fold cross-validation 
## # A tibble: 5 × 4
##   splits               id    .metrics          .notes          
##   &lt;list&gt;               &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          
## 1 &lt;split [25150/6288]&gt; Fold1 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 1]&gt;
## 2 &lt;split [25150/6288]&gt; Fold2 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 1]&gt;
## 3 &lt;split [25150/6288]&gt; Fold3 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 1]&gt;
## 4 &lt;split [25151/6287]&gt; Fold4 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 1]&gt;
## 5 &lt;split [25151/6287]&gt; Fold5 &lt;tibble [12 × 6]&gt; &lt;tibble [0 × 1]&gt;
```

---


```r
tune_res$.metrics[[1]]
```

```
## # A tibble: 12 × 6
##     mtry min_n .metric .estimator .estimate .config              
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                
##  1    10    10 rmse    standard       0.687 Preprocessor1_Model01
##  2    30    10 rmse    standard       0.649 Preprocessor1_Model02
##  3    50    10 rmse    standard       0.644 Preprocessor1_Model03
##  4    70    10 rmse    standard       0.642 Preprocessor1_Model04
##  5    10    20 rmse    standard       0.697 Preprocessor1_Model05
##  6    30    20 rmse    standard       0.655 Preprocessor1_Model06
##  7    50    20 rmse    standard       0.646 Preprocessor1_Model07
##  8    70    20 rmse    standard       0.643 Preprocessor1_Model08
##  9    10    30 rmse    standard       0.701 Preprocessor1_Model09
## 10    30    30 rmse    standard       0.657 Preprocessor1_Model10
## 11    50    30 rmse    standard       0.651 Preprocessor1_Model11
## 12    70    30 rmse    standard       0.643 Preprocessor1_Model12
```

---

Collect the mean metric across folds:


```r
estimates &lt;- collect_metrics(tune_res)

estimates
```

```
## # A tibble: 12 × 8
##     mtry min_n .metric .estimator  mean     n std_err .config              
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1    10    10 rmse    standard   0.698     5 0.00647 Preprocessor1_Model01
##  2    30    10 rmse    standard   0.656     5 0.00673 Preprocessor1_Model02
##  3    50    10 rmse    standard   0.648     5 0.00569 Preprocessor1_Model03
##  4    70    10 rmse    standard   0.646     5 0.00572 Preprocessor1_Model04
##  5    10    20 rmse    standard   0.704     5 0.00668 Preprocessor1_Model05
##  6    30    20 rmse    standard   0.662     5 0.00638 Preprocessor1_Model06
##  7    50    20 rmse    standard   0.650     5 0.00527 Preprocessor1_Model07
##  8    70    20 rmse    standard   0.647     5 0.00576 Preprocessor1_Model08
##  9    10    30 rmse    standard   0.710     5 0.00740 Preprocessor1_Model09
## 10    30    30 rmse    standard   0.664     5 0.00590 Preprocessor1_Model10
## 11    50    30 rmse    standard   0.654     5 0.00526 Preprocessor1_Model11
## 12    70    30 rmse    standard   0.649     5 0.00567 Preprocessor1_Model12
```

---

Choose best parameter:


```r
estimates %&gt;%
  mutate(min_n = factor(min_n)) %&gt;%
  ggplot(aes(x = mtry, y = mean, color = min_n)) + 
  geom_point() + 
  geom_line() + 
  labs(y = "Mean RMSE") +
  theme_bw()
```

&lt;img src="images/Tidymodels-RMSE-Comp-1.png" width="100%" /&gt;

---

There are of course also methods for helping us choose best params and final model.


```r
best_rmse &lt;- tune_res %&gt;% select_best(metric = "rmse")
best_rmse
```

```
## # A tibble: 1 × 3
##    mtry min_n .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;                
## 1    70    10 Preprocessor1_Model04
```

See also `?select_by_one_std_err`.


```r
mod_rf_final &lt;- finalize_model(mod_rf_spec, best_rmse)
mod_rf_final
```

```
## Random Forest Model Specification (regression)
## 
## Main Arguments:
##   mtry = 70
##   trees = 100
##   min_n = 10
## 
## Computational engine: ranger
```

---


```r
mod_rf_final %&gt;%
  fit(fb ~ ., data = blogs_tr) %&gt;%
  predict(new_data = blogs_te) %&gt;%
  mutate(truth = blogs_te$fb) %&gt;%
  head(10)
```


```
## # A tibble: 10 × 2
##    .pred truth
##    &lt;dbl&gt; &lt;dbl&gt;
##  1 0.584 0.693
##  2 0.584 0.693
##  3 0.238 0    
##  4 1.79  2.30 
##  5 1.79  2.30 
##  6 0.145 0    
##  7 3.33  1.10 
##  8 0.719 1.39 
##  9 2.40  3.09 
## 10 1.14  0.693
```

---

### Book (WIP?)

&lt;img src="images/tmwr.png" style="width: 45%" /&gt;

https://www.tmwr.org/

---

class: section-slide

# `infer`: Tidy Statistics

---

### Statistical Q1

Is there a relation between posts published on Sundays and blogger hand dominance, where hand dominance is totally made up? 😬




```r
pub_vs_hand &lt;- blogs_fb %&gt;%
  select(sunday, hand) %&gt;%
  table()

pub_vs_hand
```

```
##       hand
## sunday  left right
##     NS  4817 42921
##     S    495  4164
```


```r
prop.table(pub_vs_hand, margin = 1)
```

```
##       hand
## sunday      left     right
##     NS 0.1009049 0.8990951
##     S  0.1062460 0.8937540
```

---

### Statistical Q2

Is there a difference in feedback between posts published on Sundays and posts published on another day?


```r
blogs_fb %&gt;%
  group_by(sunday) %&gt;% summarise(avg = mean(fb), sd = sd(fb), n = n())
```

```
## # A tibble: 2 × 4
##   sunday   avg    sd     n
##   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 NS     0.645  1.13 47738
## 2 S      0.605  1.12  4659
```

---

### Same Problem!

Varied interface, varied output.


```r
prop.test(pub_vs_hand[,1], rowSums(pub_vs_hand))
```

```
## 
## 	2-sample test for equality of proportions with continuity correction
## 
## data:  pub_vs_hand[, 1] out of rowSums(pub_vs_hand)
## X-squared = 1.2712, df = 1, p-value = 0.2595
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  -0.014710610  0.004028538
## sample estimates:
##    prop 1    prop 2 
## 0.1009049 0.1062460
```

---


```r
t.test(fb ~ sunday, data = blogs_fb)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  fb by sunday
## t = 2.3606, df = 5638.9, p-value = 0.01828
## alternative hypothesis: true difference in means between group NS and group S is not equal to 0
## 95 percent confidence interval:
##  0.006863705 0.074103874
## sample estimates:
## mean in group NS  mean in group S 
##        0.6454439        0.6049601
```

---

### The `generics::tidy()` Approach

(Also available when you load several other packages, like `broom` and `yardstick`)


```r
tidy(prop.test(pub_vs_hand[,1], rowSums(pub_vs_hand)))
```

```
## # A tibble: 1 × 9
##   estimate1 estimate2 statistic p.value parameter conf.low conf.high method     
##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      
## 1     0.101     0.106      1.27   0.260         1  -0.0147   0.00403 2-sample t…
## # … with 1 more variable: alternative &lt;chr&gt;
```


```r
tidy(t.test(fb ~ sunday, data = blogs_fb))
```

```
## # A tibble: 1 × 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1   0.0405     0.645     0.605      2.36  0.0183     5639.  0.00686    0.0741
## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;
```

---

### The `infer` Approach

&gt; infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework

4 main verbs for a typical flow:

* `specify()` - dependent/independent variables, formula
* `hypothesize()` - declare the null hypothesis
* `generate()` - generate data reflecting the null hypothesis (the permutation/bootstrap approach)
* `calculate()` - calculate a distribution of statistics from the generated data, from which you can extract conclusion based on a p-value for example

---

### `infer` Diff in Proportions Test

Get the observed statistic (here manually in order to not confuse you, there *is* a way via `infer`):


```r
pub_vs_hand
```

```
##       hand
## sunday  left right
##     NS  4817 42921
##     S    495  4164
```


```r
p_NS &lt;- pub_vs_hand[1, 1] / (sum(pub_vs_hand[1, ]))
p_S &lt;- pub_vs_hand[2, 1] / (sum(pub_vs_hand[2, ]))
obs_diff &lt;- p_NS - p_S
obs_diff
```

```
## [1] -0.005341036
```

---

Get distribution of the difference in proportions under null hypothesis


```r
diff_null_perm &lt;- blogs_fb %&gt;%
  specify(hand ~ sunday, success = "left") %&gt;%
  hypothesize(null = "independence") %&gt;%
  generate(reps = 200, type = "permute") %&gt;%
  calculate(stat = "diff in props", order = c("NS", "S"))

diff_null_perm
```

```
## Response: hand (factor)
## Explanatory: sunday (factor)
## Null Hypothesis: independence
## # A tibble: 200 × 2
##    replicate      stat
##        &lt;int&gt;     &lt;dbl&gt;
##  1         1  0.00338 
##  2         2 -0.000629
##  3         3  0.00432 
##  4         4 -0.0126  
##  5         5 -0.00793 
##  6         6  0.00338 
##  7         7  0.00620 
##  8         8  0.0133  
##  9         9  0.000313
## 10        10  0.00526 
## # … with 190 more rows
```

---

Visualize the permuted difference null distribution and the p-value


```r
visualize(diff_null_perm) +
  shade_p_value(obs_stat = obs_diff, direction = "two_sided")
```

&lt;img src="images/Diff-in-Props-Null-1.png" width="50%" /&gt;

---

Get the actual p-value:


```r
diff_null_perm %&gt;% 
  get_p_value(obs_stat = obs_diff, direction = "two_sided")
```

```
## # A tibble: 1 × 1
##   p_value
##     &lt;dbl&gt;
## 1    0.21
```

---

### `infer` t Test (independent samples)

Get the observed statistic (here via `infer`):


```r
obs_t &lt;- blogs_fb %&gt;%
  specify(fb ~ sunday) %&gt;%
  calculate(stat = "t", order = c("NS", "S"))
obs_t
```

```
## Response: fb (numeric)
## Explanatory: sunday (factor)
## # A tibble: 1 × 1
##    stat
##   &lt;dbl&gt;
## 1  2.36
```


---

Get distribution of the t statistic under null hypothesis


```r
t_null_perm &lt;- blogs_fb %&gt;%
  specify(fb ~ sunday) %&gt;%
  hypothesize(null = "independence") %&gt;%
  generate(reps = 100, type = "permute") %&gt;%
  calculate(stat = "t", order = c("NS", "S"))

t_null_perm
```

```
## Response: fb (numeric)
## Explanatory: sunday (factor)
## Null Hypothesis: independence
## # A tibble: 100 × 2
##    replicate   stat
##        &lt;int&gt;  &lt;dbl&gt;
##  1         1  0.489
##  2         2 -0.674
##  3         3 -1.56 
##  4         4  2.42 
##  5         5 -0.619
##  6         6  0.714
##  7         7  0.276
##  8         8 -1.76 
##  9         9 -0.514
## 10        10  1.29 
## # … with 90 more rows
```

---

Visualize the permuted t statistic null distribution and the two-sided p-value


```r
visualize(t_null_perm) +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
```

&lt;img src="images/T-Null-1.png" width="50%" /&gt;

---

Get the actual p-value:


```r
t_null_perm %&gt;% 
  get_p_value(obs_stat = obs_t, direction = "two_sided")
```

```
## # A tibble: 1 × 1
##   p_value
##     &lt;dbl&gt;
## 1    0.04
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
