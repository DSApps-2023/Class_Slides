<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Modeling in the Tidyverse</title>
    <meta charset="utf-8" />
    <meta name="author" content="Giora Simchoni" />
    <meta name="date" content="2021-12-30" />
    <script src="u1_d05-modeling_files/header-attrs-2.11/header-attrs.js"></script>
    <head>
      <link rel="icon" href="../DSApps_logo.jpg" type="image/jpg"> 
      <link rel="shortcut icon" href="../DSApps_logo.jpg" type="image/jpg">
    </head>
    <link rel="stylesheet" href="../slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: logo-slide

---

class: title-slide

## Modeling in the Tidyverse

### Applications of Data Science - Class 5

### Giora Simchoni

#### `gsimchoni@gmail.com and add #dsapps in subject`

### Stat. and OR Department, TAU
### 2021-12-30

---



layout: true

&lt;div class="my-footer"&gt;
  &lt;span&gt;
    &lt;a href="https://dsapps-2022.github.io/Class_Slides/" target="_blank"&gt;Applications of Data Science
    &lt;/a&gt;
  &lt;/span&gt;
&lt;/div&gt;

---



class: section-slide

# The Problem

---

### Inconsistency, Inextensibility


```r
n &lt;- 10000
x1 &lt;- runif(n)
x2 &lt;- runif(n)
t &lt;- 1 + 2 * x1 + 3 * x2
y &lt;- rbinom(n, 1, 1 / (1 + exp(-t)))
```


```r
glm(y ~ x1 + x2, family = "binomial")
```


```r
glmnet(as.matrix(cbind(x1, x2)), as.factor(y), family = "binomial")
```


```r
randomForest(as.factor(y) ~ x1 + x2)
```



```r
gbm(y ~ x1 + x2, data = data.frame(x1 = x1, x2 = x2, y = y))
```

üò±

---

### Compare this with `sklearn`


```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier,
  GradientBoostingClassifier

LogisticRegression(penalty='none').fit(X, y)

LogisticRegression(penalty='l2', C=0.001).fit(X, y)

RandomForestClassifier(n_estimators=100).fit(X, y)

GradientBoostingClassifier(n_estimators=100).fit(X, y)
```

---

class: section-slide

# Detour: A Regression Problem

---

### Hungarian Blogs: Predicting Feedback

- Dataset was published as part of the [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/BlogFeedback) initiative
- Comes from [Buza 2014](http://www.cs.bme.hu/~buza/pdfs/gfkl2012_blogs.pdf)
- 280 numeric heavily engineered features on blogs and posts published in the last 72 hours
- can we predict no. of comments in the next 24 hours?

&lt;img src="images/hungarian_blog.png" style="width: 70%" /&gt;

---

The raw data has over 50K rows: for each blog features like total comments until base time, weekday, words, etc.

We will be predicting `log(fb)` based on all features, no missing values:


```r
blogs_fb &lt;- read_csv("~/BlogFeedback/blogData_train.csv", col_names = FALSE)

blogs_fb &lt;- blogs_fb %&gt;%
  rename(fb = X281, blog_len = X62, sunday = X276) %&gt;%
  mutate(fb = log(fb + 1))

dim(blogs_fb)
```

```
## [1] 52397   281
```

---


```r
glimpse(blogs_fb)
```

```
## Rows: 52,397
## Columns: 281
## $ X1       &lt;dbl&gt; 40.30467, 40.30467, 40.30467, 40.30467, 40.30467, 40.30467, 4~
## $ X2       &lt;dbl&gt; 53.84566, 53.84566, 53.84566, 53.84566, 53.84566, 53.84566, 5~
## $ X3       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X4       &lt;dbl&gt; 401, 401, 401, 401, 401, 401, 401, 401, 401, 401, 401, 401, 4~
## $ X5       &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1~
## $ X6       &lt;dbl&gt; 15.52416, 15.52416, 15.52416, 15.52416, 15.52416, 15.52416, 1~
## $ X7       &lt;dbl&gt; 32.44188, 32.44188, 32.44188, 32.44188, 32.44188, 32.44188, 3~
## $ X8       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X9       &lt;dbl&gt; 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 3~
## $ X10      &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3~
## $ X11      &lt;dbl&gt; 14.04423, 14.04423, 14.04423, 14.04423, 14.04423, 14.04423, 1~
## $ X12      &lt;dbl&gt; 32.61542, 32.61542, 32.61542, 32.61542, 32.61542, 32.61542, 3~
## $ X13      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X14      &lt;dbl&gt; 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 3~
## $ X15      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2~
## $ X16      &lt;dbl&gt; 34.56757, 34.56757, 34.56757, 34.56757, 34.56757, 34.56757, 3~
## $ X17      &lt;dbl&gt; 48.47518, 48.47518, 48.47518, 48.47518, 48.47518, 48.47518, 4~
## $ X18      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X19      &lt;dbl&gt; 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 3~
## $ X20      &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1~
## $ X21      &lt;dbl&gt; 1.479934, 1.479934, 1.479934, 1.479934, 1.479934, 1.479934, 1~
## $ X22      &lt;dbl&gt; 46.18691, 46.18691, 46.18691, 46.18691, 46.18691, 46.18691, 4~
## $ X23      &lt;dbl&gt; -356, -356, -356, -356, -356, -356, -356, -356, -356, -356, -~
## $ X24      &lt;dbl&gt; 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 377, 3~
## $ X25      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X26      &lt;dbl&gt; 1.076167, 1.076167, 1.076167, 1.076167, 1.076167, 1.076167, 1~
## $ X27      &lt;dbl&gt; 1.795416, 1.795416, 1.795416, 1.795416, 1.795416, 1.795416, 1~
## $ X28      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X29      &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1~
## $ X30      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X31      &lt;dbl&gt; 0.4004914, 0.4004914, 0.4004914, 0.4004914, 0.4004914, 0.4004~
## $ X32      &lt;dbl&gt; 1.078097, 1.078097, 1.078097, 1.078097, 1.078097, 1.078097, 1~
## $ X33      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X34      &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9~
## $ X35      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X36      &lt;dbl&gt; 0.3775594, 0.3775594, 0.3775594, 0.3775594, 0.3775594, 0.3775~
## $ X37      &lt;dbl&gt; 1.07421, 1.07421, 1.07421, 1.07421, 1.07421, 1.07421, 1.07421~
## $ X38      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X39      &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9~
## $ X40      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X41      &lt;dbl&gt; 0.972973, 0.972973, 0.972973, 0.972973, 0.972973, 0.972973, 0~
## $ X42      &lt;dbl&gt; 1.704671, 1.704671, 1.704671, 1.704671, 1.704671, 1.704671, 1~
## $ X43      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X44      &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1~
## $ X45      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X46      &lt;dbl&gt; 0.02293202, 0.02293202, 0.02293202, 0.02293202, 0.02293202, 0~
## $ X47      &lt;dbl&gt; 1.521174, 1.521174, 1.521174, 1.521174, 1.521174, 1.521174, 1~
## $ X48      &lt;dbl&gt; -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -8, -~
## $ X49      &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9~
## $ X50      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X51      &lt;dbl&gt; 2, 6, 6, 2, 3, 6, 6, 3, 30, 30, 0, 0, 30, 0, 51, 10, 32, 10, ~
## $ X52      &lt;dbl&gt; 2, 2, 2, 2, 1, 0, 0, 1, 27, 27, 0, 0, 30, 0, 51, 10, 2, 10, 5~
## $ X53      &lt;dbl&gt; 0, 4, 4, 0, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 30, 0, 0, 51,~
## $ X54      &lt;dbl&gt; 2, 5, 5, 2, 2, 5, 5, 2, 2, 2, 0, 0, 30, 0, 51, 10, 32, 10, 51~
## $ X55      &lt;dbl&gt; 2, -2, -2, 2, -1, -2, -2, -1, 26, 26, 0, 0, 30, 0, 51, 10, -2~
## $ X56      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 2, 3, 0, 2, 0, 3, 4, 0~
## $ X57      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 3, 0, 1, 0, 3, 1, 0~
## $ X58      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 3, 0~
## $ X59      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 2, 3, 0, 2, 0, 3, 4, 0~
## $ X60      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, -2, 1, 0, 3, 0, 0, 0, 3, -2,~
## $ X61      &lt;dbl&gt; 10, 35, 35, 10, 34, 59, 59, 34, 58, 58, 11, 35, 5, 59, 8, 14,~
## $ blog_len &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X63      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X64      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X65      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X66      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X67      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X68      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X69      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X70      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X71      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X72      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X73      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X74      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X75      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X76      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X77      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X78      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X79      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X80      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X81      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X82      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X83      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X84      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X85      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X86      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X87      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X88      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X89      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X90      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X91      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X92      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X93      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X94      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X95      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X96      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X97      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X98      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X99      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X100     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X101     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X102     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X103     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X104     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X105     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X106     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X107     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X108     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X109     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X110     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X111     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X112     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X113     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X114     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X115     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X116     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X117     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X118     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X119     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X120     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X121     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X122     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X123     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X124     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X125     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X126     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X127     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X128     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X129     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X130     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X131     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X132     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X133     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X134     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X135     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X136     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X137     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X138     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X139     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X140     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X141     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X142     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X143     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X144     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X145     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X146     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X147     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X148     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X149     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X150     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X151     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X152     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X153     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X154     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X155     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X156     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X157     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X158     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X159     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X160     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X161     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X162     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X163     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X164     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X165     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X166     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X167     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X168     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X169     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X170     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X171     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X172     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X173     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X174     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X175     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X176     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X177     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X178     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X179     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X180     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X181     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X182     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X183     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X184     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X185     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X186     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X187     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X188     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X189     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X190     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X191     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X192     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X193     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X194     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X195     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X196     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X197     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X198     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X199     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X200     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X201     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X202     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X203     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X204     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X205     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X206     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X207     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X208     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X209     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X210     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X211     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X212     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X213     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X214     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X215     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X216     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X217     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X218     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X219     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X220     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X221     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X222     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X223     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X224     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X225     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X226     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X227     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X228     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X229     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X230     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X231     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X232     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X233     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X234     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X235     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X236     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X237     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X238     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X239     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X240     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X241     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X242     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X243     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X244     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X245     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X246     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X247     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X248     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X249     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X250     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X251     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X252     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X253     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X254     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X255     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X256     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X257     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X258     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X259     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X260     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X261     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X262     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X263     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X264     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0~
## $ X265     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0~
## $ X266     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1~
## $ X267     &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X268     &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X269     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X270     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0~
## $ X271     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1~
## $ X272     &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X273     &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X274     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X275     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0~
## $ sunday   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X277     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X278     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X279     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ X280     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ fb       &lt;dbl&gt; 0.6931472, 0.0000000, 0.0000000, 0.6931472, 3.3322045, 0.0000~
```

---

See the dependent variable distribution:


```r
ggplot(blogs_fb, aes(fb)) +
  geom_histogram(fill = "red", alpha = 0.5, binwidth = 0.5) +
  theme_bw()
```

&lt;img src="images/Bench-Hist-1.png" width="100%" /&gt;

---

See it vs. say "length of post":


```r
ggplot(blogs_fb, aes(log(blog_len), fb)) +
  geom_point(color = "red", alpha = 0.5) +
  theme_bw()
```

&lt;img src="images/Bench-Age-Equipment-1.png" width="100%" /&gt;

---

class: section-slide

# End of Detour

---

# WARNING

.warning[
üí° What you're about to see is not a good modeling/prediction flow. This is just an intro to tidy modeling. Some of the issues with how things are done here will be raised, some will have to wait till later in the course.
]
---

class: section-slide

# The ~~Present~~ Past Solution: `caret`

---

### Split Data


```r
library(caret)

train_idx &lt;- createDataPartition(blogs_fb$fb,
                                 p = 0.6, list = FALSE)

blogs_tr &lt;- blogs_fb[train_idx, ]
blogs_te &lt;- blogs_fb[-train_idx, ]

library(glue)
glue("train no. of rows: {nrow(blogs_tr)}
     test no. of rows: {nrow(blogs_te)}")
```

```
## train no. of rows: 31439
## test no. of rows: 20958
```

Here you might consider some preprocessing.

`caret` has some nice documentation [here](https://topepo.github.io/caret/index.html).

---

### Tuning and Modeling

Define general methodology, e.g. 5-fold Cross-Validation:


```r
fit_control &lt;- trainControl(method = "cv", number = 5)

ridge_grid &lt;- expand.grid(alpha=0, lambda = 10^seq(-3, 1, length = 50))
lasso_grid &lt;- expand.grid(alpha=1, lambda = 10^seq(-3, 1, length = 50))
rf_grid &lt;- expand.grid(splitrule = "variance",
                       min.node.size = seq(10, 30, 10),
                       mtry = seq(10, 50, 20))

mod_ridge &lt;- train(fb ~ ., data = blogs_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = ridge_grid,
                metric = "RMSE")

mod_lasso &lt;- train(fb ~ ., data = blogs_tr, method = "glmnet",
                trControl = fit_control, tuneGrid = lasso_grid,
                metric = "RMSE")

mod_rf &lt;- train(fb ~ ., data = blogs_tr, method = "ranger",
                trControl = fit_control, tuneGrid = rf_grid,
                num.trees = 50, metric = "RMSE")
```

---

### Evaluating Models


```r
mod_ridge
```

```
## glmnet 
## 
## 31439 samples
##   280 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 25151, 25151, 25151, 25152, 25151 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE       Rsquared   MAE      
##    0.001000000  0.8467994  0.4387687  0.5919345
##    0.001206793  0.8467994  0.4387687  0.5919345
##    0.001456348  0.8467994  0.4387687  0.5919345
##    0.001757511  0.8467994  0.4387687  0.5919345
##    0.002120951  0.8467994  0.4387687  0.5919345
##    0.002559548  0.8467994  0.4387687  0.5919345
##    0.003088844  0.8467994  0.4387687  0.5919345
##    0.003727594  0.8467994  0.4387687  0.5919345
##    0.004498433  0.8467994  0.4387687  0.5919345
##    0.005428675  0.8467994  0.4387687  0.5919345
##    0.006551286  0.8467994  0.4387687  0.5919345
##    0.007906043  0.8467994  0.4387687  0.5919345
##    0.009540955  0.8467994  0.4387687  0.5919345
##    0.011513954  0.8467994  0.4387687  0.5919345
##    0.013894955  0.8467994  0.4387687  0.5919345
##    0.016768329  0.8467994  0.4387687  0.5919345
##    0.020235896  0.8467994  0.4387687  0.5919345
##    0.024420531  0.8467994  0.4387687  0.5919345
##    0.029470517  0.8467994  0.4387687  0.5919345
##    0.035564803  0.8467994  0.4387687  0.5919345
##    0.042919343  0.8467994  0.4387687  0.5919345
##    0.051794747  0.8467994  0.4387687  0.5919345
##    0.062505519  0.8469941  0.4385193  0.5920923
##    0.075431201  0.8483327  0.4368035  0.5931746
##    0.091029818  0.8497365  0.4350140  0.5942909
##    0.109854114  0.8511866  0.4331812  0.5954245
##    0.132571137  0.8526964  0.4312942  0.5965880
##    0.159985872  0.8542570  0.4293735  0.5977804
##    0.193069773  0.8558905  0.4274006  0.5990308
##    0.232995181  0.8576077  0.4253740  0.6003895
##    0.281176870  0.8594459  0.4232575  0.6018988
##    0.339322177  0.8614624  0.4209874  0.6036223
##    0.409491506  0.8636834  0.4185469  0.6056161
##    0.494171336  0.8661800  0.4158526  0.6079891
##    0.596362332  0.8690192  0.4128125  0.6108028
##    0.719685673  0.8722486  0.4093735  0.6141767
##    0.868511374  0.8759209  0.4054581  0.6181927
##    1.048113134  0.8800723  0.4010029  0.6228518
##    1.264855217  0.8847217  0.3959633  0.6280963
##    1.526417967  0.8898640  0.3903287  0.6338345
##    1.842069969  0.8955002  0.3840820  0.6400043
##    2.222996483  0.9015927  0.3772582  0.6465068
##    2.682695795  0.9080955  0.3699175  0.6532459
##    3.237457543  0.9149566  0.3621446  0.6602069
##    3.906939937  0.9221217  0.3540468  0.6673374
##    4.714866363  0.9295400  0.3457492  0.6745553
##    5.689866029  0.9371697  0.3373892  0.6818613
##    6.866488450  0.9449811  0.3291073  0.6892126
##    8.286427729  0.9529569  0.3210323  0.6966290
##   10.000000000  0.9610923  0.3132701  0.7040947
## 
## Tuning parameter 'alpha' was held constant at a value of 0
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 0 and lambda = 0.05179475.
```

---


```r
mod_lasso
```

```
## glmnet 
## 
## 31439 samples
##   280 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 25151, 25151, 25152, 25151, 25151 
## Resampling results across tuning parameters:
## 
##   lambda        RMSE       Rsquared   MAE      
##    0.001000000  0.8337688  0.4554690  0.5798744
##    0.001206793  0.8341500  0.4549660  0.5802843
##    0.001456348  0.8347039  0.4542405  0.5808603
##    0.001757511  0.8355192  0.4531749  0.5817068
##    0.002120951  0.8366514  0.4517014  0.5828165
##    0.002559548  0.8377229  0.4503159  0.5837793
##    0.003088844  0.8391754  0.4484291  0.5849757
##    0.003727594  0.8412298  0.4457342  0.5865769
##    0.004498433  0.8431266  0.4432589  0.5880512
##    0.005428675  0.8451364  0.4406221  0.5893364
##    0.006551286  0.8475508  0.4374386  0.5908574
##    0.007906043  0.8503312  0.4337628  0.5929027
##    0.009540955  0.8530921  0.4301238  0.5950345
##    0.011513954  0.8557984  0.4265541  0.5970637
##    0.013894955  0.8581235  0.4235255  0.5987878
##    0.016768329  0.8604976  0.4204546  0.6007478
##    0.020235896  0.8617014  0.4190834  0.6019269
##    0.024420531  0.8629389  0.4177748  0.6032117
##    0.029470517  0.8643976  0.4163013  0.6047518
##    0.035564803  0.8661970  0.4145147  0.6067680
##    0.042919343  0.8682922  0.4125556  0.6093572
##    0.051794747  0.8707154  0.4105047  0.6126418
##    0.062505519  0.8737768  0.4080528  0.6171222
##    0.075431201  0.8772938  0.4056991  0.6226208
##    0.091029818  0.8808258  0.4045722  0.6286455
##    0.109854114  0.8855927  0.4033883  0.6363200
##    0.132571137  0.8924927  0.4013860  0.6458831
##    0.159985872  0.9024496  0.3978668  0.6578947
##    0.193069773  0.9167594  0.3913552  0.6731065
##    0.232995181  0.9372116  0.3784736  0.6923456
##    0.281176870  0.9655296  0.3528261  0.7162081
##    0.339322177  0.9923029  0.3439063  0.7365309
##    0.409491506  1.0279654  0.3290905  0.7613254
##    0.494171336  1.0731909  0.2916655  0.7902975
##    0.596362332  1.1242850  0.2885385  0.8187743
##    0.719685673  1.1298176        NaN  0.8218104
##    0.868511374  1.1298176        NaN  0.8218104
##    1.048113134  1.1298176        NaN  0.8218104
##    1.264855217  1.1298176        NaN  0.8218104
##    1.526417967  1.1298176        NaN  0.8218104
##    1.842069969  1.1298176        NaN  0.8218104
##    2.222996483  1.1298176        NaN  0.8218104
##    2.682695795  1.1298176        NaN  0.8218104
##    3.237457543  1.1298176        NaN  0.8218104
##    3.906939937  1.1298176        NaN  0.8218104
##    4.714866363  1.1298176        NaN  0.8218104
##    5.689866029  1.1298176        NaN  0.8218104
##    6.866488450  1.1298176        NaN  0.8218104
##    8.286427729  1.1298176        NaN  0.8218104
##   10.000000000  1.1298176        NaN  0.8218104
## 
## Tuning parameter 'alpha' was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.001.
```

---


```r
mod_rf
```

```
## Random Forest 
## 
## 31439 samples
##   280 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 25151, 25151, 25151, 25152, 25151 
## Resampling results across tuning parameters:
## 
##   min.node.size  mtry  RMSE       Rsquared   MAE      
##   10             10    0.6994401  0.6235614  0.4490280
##   10             30    0.6608756  0.6590346  0.4122673
##   10             50    0.6550989  0.6642348  0.4053400
##   20             10    0.7067291  0.6160797  0.4553954
##   20             30    0.6648241  0.6554564  0.4179512
##   20             50    0.6556846  0.6640163  0.4091380
##   30             10    0.7125389  0.6106795  0.4608372
##   30             30    0.6662476  0.6544456  0.4200763
##   30             50    0.6575038  0.6623432  0.4109595
## 
## Tuning parameter 'splitrule' was held constant at a value of variance
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 50, splitrule = variance
##  and min.node.size = 10.
```

---


```r
plot(mod_ridge)
```

&lt;img src="images/Ridge-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_lasso)
```

&lt;img src="images/Lasso-CV-1.png" width="80%" /&gt;

---


```r
plot(mod_rf)
```

&lt;img src="images/RF-CV-1.png" width="80%" /&gt;

---

### Comparing Models


```r
resamps &lt;- resamples(list(Ridge = mod_ridge, Lasso = mod_lasso,
                          RF = mod_rf))
summary(resamps)
```

```
## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: Ridge, Lasso, RF 
## Number of resamples: 5 
## 
## MAE 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.5865591 0.5865627 0.5926886 0.5919345 0.5968818 0.5969802    0
## Lasso 0.5656421 0.5818206 0.5835294 0.5798744 0.5839981 0.5843815    0
## RF    0.4033943 0.4046754 0.4050187 0.4053400 0.4060125 0.4075993    0
## 
## RMSE 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.8380841 0.8412998 0.8453311 0.8467994 0.8512373 0.8580448    0
## Lasso 0.8109915 0.8299759 0.8412628 0.8337688 0.8426856 0.8439281    0
## RF    0.6454345 0.6559274 0.6567492 0.6550989 0.6577902 0.6595931    0
## 
## Rsquared 
##            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## Ridge 0.4297911 0.4339563 0.4364365 0.4387687 0.4461269 0.4475327    0
## Lasso 0.4447982 0.4455008 0.4582548 0.4554690 0.4628259 0.4659653    0
## RF    0.6470535 0.6611002 0.6634669 0.6642348 0.6718058 0.6777477    0
```

---


```r
dotplot(resamps, metric = "RMSE")
```

&lt;img src="images/Caret-RMSE-Comp-1.png" width="100%" /&gt;

---

### Predicting


```r
pred_ridge &lt;- predict(mod_ridge, newdata = blogs_te)
pred_lasso &lt;- predict(mod_lasso, newdata = blogs_te)
pred_rf &lt;- predict(mod_rf, newdata = blogs_te)

rmse_ridge &lt;- RMSE(pred_ridge, blogs_te$fb)
rmse_lasso &lt;- RMSE(pred_lasso, blogs_te$fb)
rmse_rf &lt;- RMSE(pred_rf, blogs_te$fb)

glue("Test RMSE Ridge: {format(rmse_ridge, digits = 3)}
     Test RMSE Lassoe: {format(rmse_lasso, digits = 3)}
     Test RMSE RF: {format(rmse_rf, digits = 3)}")
```

```
## Test RMSE Ridge: 0.85
## Test RMSE Lassoe: 0.835
## Test RMSE RF: 0.641
```

---


```r
bind_rows(
  tibble(method = "Ridge", pred = pred_ridge, true = blogs_te$fb),
  tibble(method = "Lasso", pred = pred_lasso, true = blogs_te$fb),
  tibble(method = "RF", pred = pred_rf, true = blogs_te$fb)) %&gt;%
  ggplot(aes(true, pred)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ factor(method, levels = c("Ridge", "Lasso", "RF"))) +
  theme_bw()
```

&lt;img src="images/Caret-Pred-vs-True-1.png" width="100%" /&gt;

---

class: section-slide

# The ~~Future~~ Present Solution: `tidymodels`

#### Inspired by [Julia Silge](https://juliasilge.com/blog/intro-tidymodels/)

---

### Packages under tidymodels

- `parsnip`: tidy `caret`
- `dials` and `tune`: specifying and tuning model parameters
- `rsample`: sampling, data partitioning
- `recipes`, `embed`, `themis`: preprocessing and creating model matrices
- `infer`: tidy statistics
- `yardstick`: measuring models performance
- `broom`: convert models output into tidy tibbles

And [more](https://www.tidymodels.org/).

.warning[
‚ö†Ô∏è All `tidymodels` packages are under development!
]

---

### Split Data

The `initial_split()` function is from the `rsample` package:


```r
library(tidymodels)

blogs_split_obj &lt;- blogs_fb %&gt;%
  initial_split(prop = 0.6)

blogs_tr &lt;- training(blogs_split_obj)
blogs_te &lt;- testing(blogs_split_obj)

glue("train no. of rows: {nrow(blogs_tr)}
     test no. of rows: {nrow(blogs_te)}")
```

```
## train no. of rows: 31438
## test no. of rows: 20959
```

```r
print(blogs_split_obj)
```

```
## &lt;Analysis/Assess/Total&gt;
## &lt;31438/20959/52397&gt;
```

---

### Preprocess .font80percent[(but we're not gonna use it)]

The `recipe()` function is from the `recipes` package. It allows you to specify a python-like pipe you can later apply to any dataset, including all preprocessing steps:


```r
blogs_rec &lt;- recipe(fb ~ ., data = blogs_tr)
blogs_rec
```

```
## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor        280
```

`recipes` contains more preprocessing [`step_`s](https://tidymodels.github.io/recipes/reference/index.html) than you imagine:


```r
blogs_rec &lt;-  blogs_rec %&gt;%
  step_normalize(all_numeric())
```

---

After you have your `recipe` you need to `prep()` materials...


```r
blogs_rec &lt;- blogs_rec %&gt;% prep(blogs_tr)

blogs_rec
```

```
## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor        280
## 
## Training data contained 31438 data points and no missing data.
## 
## Operations:
## 
## Centering and scaling for X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X... [trained]
```

At this point our `recipe` has all necessary `sd` and `mean`s for numeric variables.

---


```r
blogs_rec$var_info
```

```
## # A tibble: 281 x 4
##    variable type    role      source  
##    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
##  1 X1       numeric predictor original
##  2 X2       numeric predictor original
##  3 X3       numeric predictor original
##  4 X4       numeric predictor original
##  5 X5       numeric predictor original
##  6 X6       numeric predictor original
##  7 X7       numeric predictor original
##  8 X8       numeric predictor original
##  9 X9       numeric predictor original
## 10 X10      numeric predictor original
## # ... with 271 more rows
```

---


```r
blogs_rec$steps[[1]]$means |&gt; head()
```

```
##          X1          X2          X3          X4          X5          X6 
##  39.5743447  46.8934480   0.4058464 339.6720847  24.7984764  15.2661519
```

```r
blogs_rec$steps[[1]]$sds |&gt; head()
```

```
##         X1         X2         X3         X4         X5         X6 
##  79.727511  62.429599   8.297409 440.376463  70.466659  32.499697
```

---

And then we `bake()` (or [`juice()`](https://tidymodels.github.io/recipes/reference/juice.html)):


```r
blogs_tr2 &lt;- blogs_rec %&gt;% bake(blogs_tr)
blogs_te2 &lt;- blogs_rec %&gt;% bake(blogs_te)

glue("mean of comments in orig training: {format(mean(blogs_tr$X51), digits = 3)}, sd: {format(sd(blogs_tr$X51), digits = 3)}
     mean of comments in baked training: {format(mean(blogs_tr2$X51), digits = 0)}, sd: {format(sd(blogs_tr2$X51), digits = 3)}")
```

```
## mean of comments in orig training: 39.5, sd: 113
## mean of comments in baked training: 0, sd: 1
```

```r
glue("mean of comments in orig testing: {format(mean(blogs_te$X51), digits = 3)}, sd: {format(sd(blogs_te$X51), digits = 3)}
     mean of comments in baked testing: {format(mean(blogs_te2$X51), digits = 0)}, sd: {format(sd(blogs_te2$X51), digits = 3)}")
```

```
## mean of comments in orig testing: 39.4, sd: 109
## mean of comments in baked testing: -0, sd: 0.963
```

---

Or you can do it all in a single pipe:


```r
blogs_rec &lt;- recipe(fb ~ ., data = blogs_tr) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  prep(blogs_tr)

blogs_tr2 &lt;- blogs_rec %&gt;% bake(blogs_tr)
blogs_te2 &lt;- blogs_rec %&gt;% bake(blogs_te)

glue("mean of comments in orig training: {format(mean(blogs_tr$X51), digits = 3)}, sd: {format(sd(blogs_tr$X51), digits = 3)}
     mean of comments in baked training: {format(mean(blogs_tr2$X51), digits = 0)}, sd: {format(sd(blogs_tr2$X51), digits = 3)}")
```

```
## mean of comments in orig training: 39.5, sd: 113
## mean of comments in baked training: 0, sd: 1
```

```r
glue("mean of comments in orig testing: {format(mean(blogs_te$X51), digits = 3)}, sd: {format(sd(blogs_te$X51), digits = 3)}
     mean of comments in baked testing: {format(mean(blogs_te2$X51), digits = 0)}, sd: {format(sd(blogs_te2$X51), digits = 3)}")
```

```
## mean of comments in orig testing: 39.4, sd: 109
## mean of comments in baked testing: -0, sd: 0.963
```

---

#### Fast Forward 10 weeks from now...


```r
rec_int_topints &lt;- recipe(pets ~ ., data = okcupid_tr) %&gt;%
  step_textfeature(essays, prefix = "t",
                   extract_functions = my_text_funs) %&gt;%
  update_role(essays, new_role = "discarded") %&gt;%
  step_mutate_at(starts_with("t_"), fn = ~ifelse(is.na(.x), 0, .x)) %&gt;%
  step_log(income, starts_with("len_"), starts_with("t_"),
           -t_essays_sent_bing, offset = 1) %&gt;%
  step_meanimpute(income) %&gt;%
  step_other(
    all_nominal(), -has_role("discarded"), -all_outcomes(),
    other = "all_else", threshold = 0.1) %&gt;%
  step_novel(
    all_nominal(), -has_role("discarded"), -all_outcomes()) %&gt;%
  step_modeimpute(all_nominal(), -has_role("discarded"), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(),
             -has_role("discarded"), one_hot = FALSE) %&gt;%
  step_interact(topint_ints) %&gt;%
  step_nzv(all_numeric(), freq_cut = 99/1) %&gt;%
  step_upsample(pets, over_ratio = 1, seed = 42)
```

---

### Modeling

For now let us use the original `blogs_tr` data.

Functions `linear_reg()` and `set_engine()` are from the `parsnip` package:


```r
mod_ridge_spec &lt;- linear_reg(mixture = 0, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet")

mod_ridge_spec
```

```
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0.001
##   mixture = 0
## 
## Computational engine: glmnet
```

---


```r
mod_ridge &lt;- mod_ridge_spec %&gt;%
  fit(fb ~ ., data = blogs_tr)

mod_ridge
```

```
## parsnip model object
## 
## Fit time:  771ms 
## 
## Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = "gaussian",      alpha = ~0) 
## 
##      Df  %Dev Lambda
## 1   276  0.00 600.90
## 2   276  2.53 547.50
## 3   276  2.76 498.90
## 4   276  3.00 454.60
## 5   276  3.27 414.20
## 6   276  3.55 377.40
## 7   276  3.85 343.90
## 8   276  4.18 313.30
## 9   276  4.53 285.50
## 10  276  4.91 260.10
## 11  276  5.31 237.00
## 12  276  5.73 216.00
## 13  276  6.18 196.80
## 14  276  6.66 179.30
## 15  276  7.17 163.40
## 16  276  7.70 148.90
## 17  276  8.26 135.60
## 18  276  8.84 123.60
## 19  276  9.45 112.60
## 20  276 10.09 102.60
## 21  276 10.74  93.49
## 22  276 11.41  85.18
## 23  276 12.11  77.61
## 24  276 12.82  70.72
## 25  276 13.54  64.44
## 26  276 14.27  58.71
## 27  276 15.01  53.50
## 28  276 15.76  48.74
## 29  276 16.51  44.41
## 30  276 17.26  40.47
## 31  276 18.01  36.87
## 32  276 18.75  33.60
## 33  276 19.49  30.61
## 34  276 20.21  27.89
## 35  276 20.93  25.41
## 36  276 21.64  23.16
## 37  276 22.34  21.10
## 38  276 23.02  19.23
## 39  276 23.69  17.52
## 40  276 24.36  15.96
## 41  276 25.01  14.54
## 42  276 25.65  13.25
## 43  276 26.28  12.07
## 44  276 26.90  11.00
## 45  276 27.52  10.02
## 46  276 28.12   9.13
## 47  276 28.72   8.32
## 48  276 29.31   7.58
## 49  276 29.90   6.91
## 50  276 30.48   6.30
## 51  276 31.05   5.74
## 52  276 31.62   5.23
## 53  276 32.17   4.76
## 54  276 32.72   4.34
## 55  276 33.26   3.95
## 56  276 33.80   3.60
## 57  276 34.32   3.28
## 58  276 34.83   2.99
## 59  276 35.33   2.72
## 60  276 35.81   2.48
## 61  276 36.29   2.26
## 62  276 36.75   2.06
## 63  276 37.19   1.88
## 64  276 37.61   1.71
## 65  276 38.02   1.56
## 66  276 38.42   1.42
## 67  276 38.79   1.29
## 68  276 39.15   1.18
## 69  276 39.49   1.07
## 70  276 39.81   0.98
## 71  276 40.11   0.89
## 72  276 40.40   0.81
## 73  276 40.67   0.74
## 74  276 40.92   0.68
## 75  276 41.16   0.62
## 76  276 41.39   0.56
## 77  276 41.60   0.51
## 78  276 41.79   0.47
## 79  276 41.98   0.42
## 80  276 42.16   0.39
## 81  276 42.32   0.35
## 82  276 42.48   0.32
## 83  276 42.63   0.29
## 84  276 42.77   0.27
## 85  276 42.91   0.24
## 86  276 43.04   0.22
## 87  276 43.17   0.20
## 88  276 43.29   0.18
## 89  276 43.41   0.17
## 90  276 43.53   0.15
## 91  276 43.64   0.14
## 92  276 43.75   0.13
## 93  276 43.86   0.12
## 94  276 43.97   0.10
## 95  276 44.07   0.10
## 96  276 44.17   0.09
## 97  276 44.27   0.08
## 98  276 44.37   0.07
## 99  276 44.46   0.07
## 100 276 44.55   0.06
```

---

In a single pipe:


```r
mod_lasso &lt;- linear_reg(mixture = 1, penalty = 0.001) %&gt;%
  set_engine(engine = "glmnet") %&gt;%
  fit(fb ~ ., data = blogs_tr)

mod_lasso
```

```
## parsnip model object
## 
## Fit time:  1.1s 
## 
## Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = "gaussian",      alpha = ~1) 
## 
##      Df  %Dev  Lambda
## 1     0  0.00 0.60090
## 2     1  4.77 0.54750
## 3     1  8.73 0.49890
## 4     3 12.52 0.45460
## 5     3 16.28 0.41420
## 6     3 19.40 0.37740
## 7     3 21.99 0.34390
## 8     3 24.14 0.31330
## 9     4 26.03 0.28550
## 10    4 28.45 0.26010
## 11    4 30.45 0.23700
## 12    4 32.12 0.21600
## 13    4 33.51 0.19680
## 14    4 34.65 0.17930
## 15    4 35.61 0.16340
## 16    4 36.40 0.14890
## 17    4 37.06 0.13560
## 18    4 37.60 0.12360
## 19    4 38.06 0.11260
## 20    4 38.43 0.10260
## 21    4 38.74 0.09349
## 22    5 39.05 0.08518
## 23    6 39.36 0.07761
## 24    6 39.62 0.07072
## 25    7 39.86 0.06444
## 26    8 40.08 0.05871
## 27    9 40.30 0.05350
## 28   11 40.49 0.04874
## 29   12 40.67 0.04441
## 30   12 40.82 0.04047
## 31   14 40.96 0.03687
## 32   15 41.09 0.03360
## 33   18 41.21 0.03061
## 34   19 41.32 0.02789
## 35   21 41.41 0.02541
## 36   25 41.51 0.02316
## 37   27 41.60 0.02110
## 38   30 41.69 0.01923
## 39   34 41.82 0.01752
## 40   41 41.99 0.01596
## 41   45 42.16 0.01454
## 42   54 42.37 0.01325
## 43   58 42.62 0.01207
## 44   62 42.83 0.01100
## 45   68 43.00 0.01002
## 46   76 43.16 0.00913
## 47   94 43.36 0.00832
## 48   94 43.58 0.00758
## 49  104 43.79 0.00691
## 50  111 43.96 0.00630
## 51  122 44.11 0.00574
## 52  134 44.27 0.00523
## 53  138 44.43 0.00476
## 54  144 44.57 0.00434
## 55  156 44.71 0.00395
## 56  164 44.93 0.00360
## 57  174 45.09 0.00328
## 58  179 45.23 0.00299
## 59  183 45.35 0.00272
## 60  190 45.45 0.00248
## 61  198 45.55 0.00226
## 62  203 45.62 0.00206
## 63  210 45.69 0.00188
## 64  206 45.75 0.00171
## 65  212 45.83 0.00156
## 66  216 45.90 0.00142
## 67  218 45.95 0.00130
## 68  219 46.00 0.00118
## 69  221 46.04 0.00108
## 70  221 46.08 0.00098
## 71  221 46.11 0.00089
## 72  224 46.15 0.00081
## 73  223 46.18 0.00074
## 74  229 46.21 0.00068
## 75  229 46.23 0.00062
## 76  232 46.27 0.00056
## 77  232 46.31 0.00051
## 78  234 46.35 0.00047
## 79  236 46.39 0.00042
## 80  237 46.42 0.00039
## 81  239 46.45 0.00035
## 82  237 46.49 0.00032
## 83  238 46.51 0.00029
## 84  238 46.53 0.00027
## 85  240 46.55 0.00024
## 86  240 46.57 0.00022
## 87  242 46.58 0.00020
## 88  244 46.60 0.00018
## 89  248 46.62 0.00017
## 90  251 46.63 0.00015
## 91  252 46.64 0.00014
## 92  253 46.65 0.00013
## 93  258 46.65 0.00012
## 94  261 46.66 0.00010
## 95  261 46.66 0.00010
## 96  261 46.67 0.00009
## 97  261 46.67 0.00008
## 98  262 46.68 0.00007
## 99  262 46.68 0.00007
## 100 264 46.69 0.00006
```

---

Can also use `fit_xy()` a-la `sklearn`:


```r
mod_rf &lt;- rand_forest(mode = "regression", mtry = 50, trees = 50, min_n = 10) %&gt;%
  set_engine("ranger") %&gt;%
  fit_xy(x = blogs_tr[, -281],
         y = blogs_tr$fb)

mod_rf
```

```
## parsnip model object
## 
## Fit time:  43.8s 
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~50,      x), num.trees = ~50, min.node.size = min_rows(~10, x), num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) 
## 
## Type:                             Regression 
## Number of trees:                  50 
## Sample size:                      31438 
## Number of independent variables:  280 
## Mtry:                             50 
## Target node size:                 10 
## Variable importance mode:         none 
## Splitrule:                        variance 
## OOB prediction error (MSE):       0.4372567 
## R squared (OOB):                  0.6598233
```

---

Notice how easy it is to get the model's results in a tidy way using the `tidy()` function:


```r
tidy(mod_ridge)
```

```
## # A tibble: 281 x 3
##    term         estimate penalty
##    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)  0.681      0.001
##  2 X1           0.00120    0.001
##  3 X2           0.00124    0.001
##  4 X3          -0.00152    0.001
##  5 X4           0.000314   0.001
##  6 X5           0.000230   0.001
##  7 X6           0.00211    0.001
##  8 X7          -0.000994   0.001
##  9 X8           0.148      0.001
## 10 X9          -0.000129   0.001
## # ... with 271 more rows
```

---

### Predicting


```r
results_test &lt;- mod_ridge %&gt;%
  predict(new_data = blogs_te, penalty = 0.001) %&gt;%
  mutate(
    truth = blogs_te$fb,
    method = "Ridge"
  ) %&gt;%
  bind_rows(mod_lasso %&gt;%
    predict(new_data = blogs_te) %&gt;%
    mutate(
      truth = blogs_te$fb,
      method = "Lasso"
  )) %&gt;%
  bind_rows(mod_rf %&gt;%
    predict(new_data = blogs_te) %&gt;%
    mutate(
      truth = blogs_te$fb,
      method = "RF"
  ))

dim(results_test)
```

```
## [1] 62877     3
```

```r
head(results_test)
```

```
## # A tibble: 6 x 3
##   .pred truth method
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
## 1 0.560  0    Ridge 
## 2 0.537  3.33 Ridge 
## 3 0.537  3.33 Ridge 
## 4 0.498  2.30 Ridge 
## 5 0.668  1.39 Ridge 
## 6 1.35   2.64 Ridge
```

---

### Comparing Models

The package `yardstick` has tons of performance [metrics](https://tidymodels.github.io/yardstick/articles/metric-types.html):


```r
results_test %&gt;%
  group_by(method) %&gt;%
  rmse(truth = truth, estimate = .pred)
```

```
## # A tibble: 3 x 4
##   method .metric .estimator .estimate
##   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 Lasso  rmse    standard       0.825
## 2 RF     rmse    standard       0.636
## 3 Ridge  rmse    standard       0.838
```

---


```r
results_test %&gt;%
  ggplot(aes(truth, .pred)) +
  geom_point(color = "red", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0) +
  facet_wrap(~ factor(method, levels = c("Ridge", "Lasso", "RF"))) +
  theme_bw()
```

&lt;img src="images/Tidymodels-Pred-vs-True-1.png" width="100%" /&gt;

---

### Tuning

Define your model spec, using `tune()` from the `tune` package for a parameter you wish to tune:


```r
mod_rf_spec &lt;- rand_forest(mode = "regression",
                           mtry = tune(),
                           min_n = tune(),
                           trees = 100) %&gt;%
  set_engine("ranger")
```

---

Define the `grid` on which you train your params, with the `dials` package:


```r
rf_grid &lt;- grid_regular(mtry(range(10, 70)), min_n(range(10, 30)),
                        levels = c(4, 3))

rf_grid
```

```
## # A tibble: 12 x 2
##     mtry min_n
##    &lt;int&gt; &lt;int&gt;
##  1    10    10
##  2    30    10
##  3    50    10
##  4    70    10
##  5    10    20
##  6    30    20
##  7    50    20
##  8    70    20
##  9    10    30
## 10    30    30
## 11    50    30
## 12    70    30
```

---

Split your data into a few folds for Cross Validation with `vfold_cv()` from the `rsample` package:


```r
cv_splits &lt;- vfold_cv(blogs_tr, v = 5)

cv_splits
```

```
## #  5-fold cross-validation 
## # A tibble: 5 x 2
##   splits               id   
##   &lt;list&gt;               &lt;chr&gt;
## 1 &lt;split [25150/6288]&gt; Fold1
## 2 &lt;split [25150/6288]&gt; Fold2
## 3 &lt;split [25150/6288]&gt; Fold3
## 4 &lt;split [25151/6287]&gt; Fold4
## 5 &lt;split [25151/6287]&gt; Fold5
```

---

Now perform cross validation with `tune_grid()` from the `tune` package:


```r
tune_res &lt;- tune_grid(mod_rf_spec,
                      recipe(fb ~ ., data = blogs_tr),
                      resamples = cv_splits,
                      grid = rf_grid,
                      metrics = metric_set(rmse))
tune_res
```


```
## # Tuning results
## # 5-fold cross-validation 
## # A tibble: 5 x 4
##   splits               id    .metrics          .notes          
##   &lt;list&gt;               &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          
## 1 &lt;split [25150/6288]&gt; Fold1 &lt;tibble [12 x 6]&gt; &lt;tibble [0 x 1]&gt;
## 2 &lt;split [25150/6288]&gt; Fold2 &lt;tibble [12 x 6]&gt; &lt;tibble [0 x 1]&gt;
## 3 &lt;split [25150/6288]&gt; Fold3 &lt;tibble [12 x 6]&gt; &lt;tibble [0 x 1]&gt;
## 4 &lt;split [25151/6287]&gt; Fold4 &lt;tibble [12 x 6]&gt; &lt;tibble [0 x 1]&gt;
## 5 &lt;split [25151/6287]&gt; Fold5 &lt;tibble [12 x 6]&gt; &lt;tibble [0 x 1]&gt;
```

---


```r
tune_res$.metrics[[1]]
```

```
## # A tibble: 12 x 6
##     mtry min_n .metric .estimator .estimate .config              
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                
##  1    10    10 rmse    standard       0.687 Preprocessor1_Model01
##  2    30    10 rmse    standard       0.649 Preprocessor1_Model02
##  3    50    10 rmse    standard       0.644 Preprocessor1_Model03
##  4    70    10 rmse    standard       0.642 Preprocessor1_Model04
##  5    10    20 rmse    standard       0.697 Preprocessor1_Model05
##  6    30    20 rmse    standard       0.655 Preprocessor1_Model06
##  7    50    20 rmse    standard       0.646 Preprocessor1_Model07
##  8    70    20 rmse    standard       0.643 Preprocessor1_Model08
##  9    10    30 rmse    standard       0.701 Preprocessor1_Model09
## 10    30    30 rmse    standard       0.657 Preprocessor1_Model10
## 11    50    30 rmse    standard       0.651 Preprocessor1_Model11
## 12    70    30 rmse    standard       0.643 Preprocessor1_Model12
```

---

Collect the mean metric across folds:


```r
estimates &lt;- collect_metrics(tune_res)

estimates
```

```
## # A tibble: 12 x 8
##     mtry min_n .metric .estimator  mean     n std_err .config              
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1    10    10 rmse    standard   0.698     5 0.00647 Preprocessor1_Model01
##  2    30    10 rmse    standard   0.656     5 0.00673 Preprocessor1_Model02
##  3    50    10 rmse    standard   0.648     5 0.00569 Preprocessor1_Model03
##  4    70    10 rmse    standard   0.646     5 0.00572 Preprocessor1_Model04
##  5    10    20 rmse    standard   0.704     5 0.00668 Preprocessor1_Model05
##  6    30    20 rmse    standard   0.662     5 0.00638 Preprocessor1_Model06
##  7    50    20 rmse    standard   0.650     5 0.00527 Preprocessor1_Model07
##  8    70    20 rmse    standard   0.647     5 0.00576 Preprocessor1_Model08
##  9    10    30 rmse    standard   0.710     5 0.00740 Preprocessor1_Model09
## 10    30    30 rmse    standard   0.664     5 0.00590 Preprocessor1_Model10
## 11    50    30 rmse    standard   0.654     5 0.00526 Preprocessor1_Model11
## 12    70    30 rmse    standard   0.649     5 0.00567 Preprocessor1_Model12
```

---

Choose best paramter:


```r
estimates %&gt;%
  mutate(min_n = factor(min_n)) %&gt;%
  ggplot(aes(x = mtry, y = mean, color = min_n)) + 
  geom_point() + 
  geom_line() + 
  labs(y = "Mean RMSE") +
  theme_bw()
```

&lt;img src="images/Tidymodels-RMSE-Comp-1.png" width="100%" /&gt;

---

There are of course also methods for helping us choose best params and final model.


```r
best_rmse &lt;- tune_res %&gt;% select_best(metric = "rmse")
best_rmse
```

```
## # A tibble: 1 x 3
##    mtry min_n .config              
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;                
## 1    70    10 Preprocessor1_Model04
```

See also `?select_by_one_std_err`.


```r
mod_rf_final &lt;- finalize_model(mod_rf_spec, best_rmse)
mod_rf_final
```

```
## Random Forest Model Specification (regression)
## 
## Main Arguments:
##   mtry = 70
##   trees = 100
##   min_n = 10
## 
## Computational engine: ranger
```

---


```r
mod_rf_final %&gt;%
  fit(fb ~ ., data = blogs_tr) %&gt;%
  predict(new_data = blogs_te) %&gt;%
  mutate(truth = blogs_te$fb)
```

```
## # A tibble: 20,959 x 2
##    .pred truth
##    &lt;dbl&gt; &lt;dbl&gt;
##  1 0.858 0.693
##  2 0.337 0    
##  3 0.147 0    
##  4 2.17  2.30 
##  5 3.37  1.10 
##  6 2.33  2.64 
##  7 1.48  2.77 
##  8 1.48  2.77 
##  9 2.33  2.64 
## 10 1.71  3.09 
## # ... with 20,949 more rows
```
---

class: section-slide

# `infer`: Tidy Statistics

---

### Statistical Q1

Is there a relation between posts published on Sundays and blogger hand dominance, where hand dominance is totally made up? üò¨




```r
pub_vs_hand &lt;- blogs_fb %&gt;%
  select(sunday, hand) %&gt;%
  table()

pub_vs_hand
```

```
##       hand
## sunday  left right
##     NS  4842 42896
##     S    456  4203
```


```r
prop.table(pub_vs_hand, margin = 1)
```

```
##       hand
## sunday       left      right
##     NS 0.10142863 0.89857137
##     S  0.09787508 0.90212492
```

---

### Statistical Q2

Is there a difference in feedback between posts published on Sundays and posts published on another day?


```r
blogs_fb %&gt;%
  group_by(sunday) %&gt;% summarise(avg = mean(fb), sd = sd(fb), n = n())
```

```
## # A tibble: 2 x 4
##   sunday   avg    sd     n
##   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 NS     0.645  1.13 47738
## 2 S      0.605  1.12  4659
```

---

### Same Problem!

Varied interface, varied output.


```r
prop.test(pub_vs_hand[,1], rowSums(pub_vs_hand))
```

```
## 
## 	2-sample test for equality of proportions with continuity correction
## 
## data:  pub_vs_hand[, 1] out of rowSums(pub_vs_hand)
## X-squared = 0.5513, df = 1, p-value = 0.4578
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  -0.005516108  0.012623209
## sample estimates:
##     prop 1     prop 2 
## 0.10142863 0.09787508
```

---


```r
t.test(fb ~ sunday, data = blogs_fb)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  fb by sunday
## t = 2.3606, df = 5638.9, p-value = 0.01828
## alternative hypothesis: true difference in means between group NS and group S is not equal to 0
## 95 percent confidence interval:
##  0.006863705 0.074103874
## sample estimates:
## mean in group NS  mean in group S 
##        0.6454439        0.6049601
```

---

### The `generics::tidy()` Approach

(Also available when you load several other packages, like `broom` and `yardstick`)


```r
tidy(prop.test(pub_vs_hand[,1], rowSums(pub_vs_hand)))
```

```
## # A tibble: 1 x 9
##   estimate1 estimate2 statistic p.value parameter conf.low conf.high method     
##       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      
## 1     0.101    0.0979     0.551   0.458         1 -0.00552    0.0126 2-sample t~
## # ... with 1 more variable: alternative &lt;chr&gt;
```


```r
tidy(t.test(fb ~ sunday, data = blogs_fb))
```

```
## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1   0.0405     0.645     0.605      2.36  0.0183     5639.  0.00686    0.0741
## # ... with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;
```

---

### The `infer` Approach

&gt; infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework

4 main verbs for a typical flow:

* `specify()` - dependent/independent variables, formula
* `hypothesize()` - declare the null hypothesis
* `generate()` - generate data reflecting the null hypothesis (the permutation/bootstrap approach)
* `calculate()` - calculate a distribution of statistics from the generated data, from which you can extract conclusion based on a p-value for example

---

### `infer` Diff in Proportions Test

Get the observed statistic (here manually in order to not confuse you, there *is* a way via `infer`):


```r
pub_vs_hand
```

```
##       hand
## sunday  left right
##     NS  4842 42896
##     S    456  4203
```


```r
p_NS &lt;- pub_vs_hand[1, 1] / (sum(pub_vs_hand[1, ]))
p_S &lt;- pub_vs_hand[2, 1] / (sum(pub_vs_hand[2, ]))
obs_diff &lt;- p_NS - p_S
obs_diff
```

```
## [1] 0.003553551
```

---

Get distribution of the difference in proportions under null hypothesis


```r
diff_null_perm &lt;- blogs_fb %&gt;%
  specify(hand ~ sunday, success = "left") %&gt;%
  hypothesize(null = "independence") %&gt;%
  generate(reps = 200, type = "permute") %&gt;%
  calculate(stat = "diff in props", order = c("NS", "S"))

diff_null_perm
```

```
## Response: hand (factor)
## Explanatory: sunday (factor)
## Null Hypothesis: independence
## # A tibble: 200 x 2
##    replicate       stat
##        &lt;int&gt;      &lt;dbl&gt;
##  1         1  0.00332  
##  2         2  0.00308  
##  3         3  0.00143  
##  4         4  0.00214  
##  5         5 -0.00234  
##  6         6  0.00779  
##  7         7  0.0000198
##  8         8 -0.00940  
##  9         9 -0.00116  
## 10        10 -0.000687 
## # ... with 190 more rows
```

---

Visualize the permuted difference null distribution and the p-value


```r
visualize(diff_null_perm) +
  shade_p_value(obs_stat = obs_diff, direction = "two_sided")
```

&lt;img src="images/Diff-in-Props-Null-1.png" width="50%" /&gt;

---

Get the actual p-value:


```r
diff_null_perm %&gt;% 
  get_p_value(obs_stat = obs_diff, direction = "two_sided")
```

```
## # A tibble: 1 x 1
##   p_value
##     &lt;dbl&gt;
## 1    0.48
```

---

### `infer` t Test (independent samples)

Get the observed statistic (here via `infer`):


```r
obs_t &lt;- blogs_fb %&gt;%
  specify(fb ~ sunday) %&gt;%
  calculate(stat = "t", order = c("NS", "S"))
obs_t
```

```
## Response: fb (numeric)
## Explanatory: sunday (factor)
## # A tibble: 1 x 1
##    stat
##   &lt;dbl&gt;
## 1  2.36
```


---

Get distribution of the t statistic under null hypothesis


```r
t_null_perm &lt;- blogs_fb %&gt;%
  specify(fb ~ sunday) %&gt;%
  hypothesize(null = "independence") %&gt;%
  generate(reps = 100, type = "permute") %&gt;%
  calculate(stat = "t", order = c("NS", "S"))

t_null_perm
```

```
## Response: fb (numeric)
## Explanatory: sunday (factor)
## Null Hypothesis: independence
## # A tibble: 100 x 2
##    replicate    stat
##        &lt;int&gt;   &lt;dbl&gt;
##  1         1 -0.223 
##  2         2 -0.392 
##  3         3 -0.486 
##  4         4 -1.09  
##  5         5  0.0289
##  6         6 -1.99  
##  7         7  0.411 
##  8         8  0.202 
##  9         9  0.161 
## 10        10 -0.453 
## # ... with 90 more rows
```

---

Visualize the permuted t statistic null distribution and the two-sided p-value


```r
visualize(t_null_perm) +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
```

&lt;img src="images/T-Null-1.png" width="50%" /&gt;

---

Get the actual p-value:


```r
t_null_perm %&gt;% 
  get_p_value(obs_stat = obs_t, direction = "two_sided")
```

&lt;pre style="color: red;"&gt;&lt;code&gt;## Warning: Please be cautious in reporting a p-value of 0. This result is an
## approximation based on the number of `reps` chosen in the `generate()` step. See
## `?get_p_value()` for more information.
&lt;/code&gt;&lt;/pre&gt;

```
## # A tibble: 1 x 1
##   p_value
##     &lt;dbl&gt;
## 1       0
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="../libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
